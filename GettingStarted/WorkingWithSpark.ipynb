{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Spark Streaming, SQL, and ML with iguazio\n",
    "Spark users can access files, tables or streams stored on iguazio data platform through the native spark Dataframe interfaces. <br>\n",
    "iguazio drivers for Spark implement the data-source API and allow `predicate push down` (the queries pass to iguazio database which only return the relevant data), this allow accelerated and high-speed access from Spark to data stored in iguazio DB. for more details read [Spark API documentation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading a file from AWS S3 into iguazio file system  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "mkdir -p /v3io/bigdata/examples\n",
    "curl -L \"deutsche-boerse-xetra-pds.s3.amazonaws.com/2018-03-26/2018-03-26_BINS_XETR07.csv\" > $HOME/examples/stocks.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating a Spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Iguazio Integration demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "To use the Iguazio Spark connector to read or write NoSQL data in the platform, use the format method to set the DataFrame’s data-source format to the platform’s custom NoSQL data source — \"io.iguaz.v3io.spark.sql.kv\". See the following read and write examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv file using Spark DF\n",
    "You can use the custom NoSQL DataFrame inferSchema read option to automatically infer the schema of the read table from its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv('v3io://users/iguazio/examples/stocks.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the spark DF into a table in Iguazio DB\n",
    "Specify the path to the NoSQL table that is associated with the DataFrame as a fully qualified path of the format v3io://container name/data path —\n",
    "where container name is the name of the table’s parent container, and data path is the path to the data within the specified container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the DB index key using the key option (note the key must be unique)\n",
    "df.write.format(\"io.iguaz.v3io.spark.sql.kv\").mode(\"append\").option(\"key\", \"ISIN\").save(\"v3io://users/iguazio/examples/stocks_tab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a table via Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(\"v3io://users/iguazio/examples/stocks_tab\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"v3io://users/iguazio/examples/stocks_tab.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Schema-Overwrite Examples\n",
    "The following example creates a table named mytable with AttrA and AttrB attributes of type string and an AttrC attribute of type long, and then overwrites the table schema to change the type of AttrC to double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"a\", \"z\", 123),\n",
    "    (\"b\", \"y\", 456)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "df.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .save(\"v3io://users/iguazio/examples/mytable/\")\n",
    "    \n",
    "df = spark.createDataFrame([\n",
    "    (\"c\", \"x\", 32.12),\n",
    "    (\"d\", \"v\", 45.2)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "df.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .option(\"allow-overwrite-schema\", \"true\") \\\n",
    "    .save(\"v3io://users/iguazio/examples/mytable/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a partition table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples creates a partitioned “weather” table  The option(\"partition\", \"year, month, day\") write option partitions the table by the year, month, and day item attributes. As demonstrated in the following image, if you browse the container in the dashboard after running the example, you’ll see that the weather directory has year=<value>/month=<value>/day=<value> partition directories that match the written items. If you select any of the nested day partition directories, you can see the written items and their attributes. For example, the first item (with attribute values 2016, 3, 25, 6, 16, 0.00, 55) is saved to a 20163256 file in a weather/year=2016/month=3/day=25 partition directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = \"v3io://users/iguazio/examples/weather/\"\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (2016,  3, 25, 17, 18, 0.2, 62),\n",
    "    (2016,  7, 24,  7, 19, 0.0, 52),\n",
    "    (2016, 12, 24,  9, 10, 0.1, 47),\n",
    "    (2017,  5,  7, 14, 21, 0.0, 70),\n",
    "    (2017, 11,  1, 10, 15, 0.0, 34),\n",
    "    (2017, 12, 12, 16, 12, 0.0, 47),\n",
    "    (2017, 12, 24, 17, 11, 1.0, 50),\n",
    "    (2018,  1, 18, 17, 10, 2.0, 45),\n",
    "    (2018,  5, 20, 21, 20, 0.0, 59),\n",
    "    (2018, 11,  1, 11, 11, 0.1, 65)\n",
    "], [\"year\", \"month\", \"day\", \"hour\", \"degrees_cel\", \"rain_ml\", \"humidity_per\"])\n",
    "df_with_key = df.withColumn(\n",
    "    \"time\", concat(df[\"year\"], df[\"month\"], df[\"day\"], df[\"hour\"]))\n",
    "df_with_key.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"time\") \\\n",
    "    .option(\"partition\", \"year, month, day, hour\") \\\n",
    "    .save(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from partition table\n",
    "Following is the output of the example’s show commands for each read. The filtered results are gathered by scanning only the partition directories that match the filter criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full table read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path)\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### month < 7 filter — retrieve all data for the first six months of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month > 6\")\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### month == 12 AND day == 24 filter — retrieve all hours on Dec 24 each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month == 12 AND day == 24\")\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### month > 6 AND hour >= 8 AND hour <= 20 filter — retrieve 08:00–20:00 data for every day in the last six months of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month < 7 AND hour >= 8 AND hour <= 20\")\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional update\n",
    "This example demonstrates how to conditionally update NoSQL table items by using the condition write option. Each write call in the example is followed by matching read and show calls to read and display the value of the updated item in the target table after the write operation.\n",
    "\n",
    "The first write command writes an item (row) to a “cars” table . The item’s reg_license primary-key (identity-column) attribute is set to 7843321, the mode attribute is set to “Honda”, and the odometer attribute is set to 29321. The overwrite save mode is used to overwrite the table if it already exists and create it otherwise. Reading the item from the table produces this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 29321)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .mode(\"overwrite\").save(\"v3io://users/iguazio/examples/cars/\")\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(\"v3io://bigdata/examples/cars/\")\n",
    "readDF.show()\n",
    "\n",
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 31718)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .option(\"condition\", \"${odometer} > odometer\") \\\n",
    "    .mode(\"append\").save(\"v3io://users/iguazio/examples/cars/\")\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(\"v3io://users/iguazio/examples/cars/\")\n",
    "readDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL queries (using Presto)\n",
    "## Reading the stock_tab table using SQL after being written by Spark DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only once (load SQL magic)\n",
    "%load_ext sql\n",
    "%config SqlMagic.autocommit=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from v3io.users.\"/iguazio/examples/stocks_tab\" where tradedvolume > 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Data\n",
    "When you are done - cleaning the directory will be done by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unmark the comment\n",
    "# !rm -rf $HOME/examples/*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
