{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark\n",
    "Spark users can access files, tables or streams stored on iguazio data platform through the native spark Dataframe interfaces. <br>\n",
    "iguazio drivers for Spark implement the data-source API and allow `predicate push down` (the queries pass to iguazio database which only return the relevant data), this allow accelerated and high-speed access from Spark to data stored in iguazio DB. for more details read [Spark API documentation]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading a file from AWS S3 into iguazio file system  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  861k  100  861k    0     0   662k      0  0:00:01  0:00:01 --:--:--  663k\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "mkdir -p /v3io/${V3IO_HOME}/examples\n",
    "curl -L \"iguazio-sample-data.s3.amazonaws.com/2018-03-26_BINS_XETR08.csv\" > /v3io/${V3IO_HOME}/examples/stocks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating a Spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Working with Spark notebook\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "To use the Iguazio Spark connector to read or write NoSQL data in the platform, use the format method to set the DataFrame’s data-source format to the platform’s custom NoSQL data source — \"io.iguaz.v3io.spark.sql.kv\". See the following read and write examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv file using Spark DF\n",
    "You can use the custom NoSQL DataFrame inferSchema read option to automatically infer the schema of the read table from its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+--------+----------+----------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|Currency|SecurityID|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|CH0038389992|    BBZA|BB BIOTECH NAM.  ...|Common stock|     EUR|   2504244|2018-03-26|08:00|      56.4|    56.4|    56.4|    56.4|         320|             4|\n",
      "|CH0038863350|    NESR|NESTLE NAM.      ...|Common stock|     EUR|   2504245|2018-03-26|08:00|     63.04|   63.06|      63|   63.06|         314|             3|\n",
      "|LU0378438732|    C001|COMSTAGE-DAX UCIT...|         ETF|     EUR|   2504271|2018-03-26|08:00|    113.42|  113.42|  113.42|  113.42|         100|             1|\n",
      "|LU0411075020|    DBPD|XTR.SHORTDAX X2 D...|         ETF|     EUR|   2504272|2018-03-26|08:00|    4.1335|  4.1335|  4.1295|    4.13|      102993|             8|\n",
      "|LU0838782315|    XDDX|   XTR.DAX INCOME 1D|         ETF|     EUR|   2504277|2018-03-26|08:00|    105.14|   105.2|  105.14|   105.2|         239|             3|\n",
      "|DE000A0DJ6J9|     S92|SMA SOLAR TECHNOL.AG|Common stock|     EUR|   2504287|2018-03-26|08:00|     55.65|   55.65|   55.65|   55.65|         543|             3|\n",
      "|DE000A0D6554|    NDX1|      NORDEX SE O.N.|Common stock|     EUR|   2504290|2018-03-26|08:00|     8.182|    8.21|   8.174|    8.21|       10941|             8|\n",
      "|DE000A0F5UE8|    EXXU|IS.DJ CHINA OFFS....|         ETF|     EUR|   2504302|2018-03-26|08:00|     47.52|   47.52|   47.52|   47.52|         420|             1|\n",
      "|DE000A0HN5C6|    DWNI|DEUTSCHE WOHNEN S...|Common stock|     EUR|   2504314|2018-03-26|08:00|      36.2|   36.24|    36.2|   36.24|         580|             5|\n",
      "|DE000A0LD2U1|     AOX|ALSTRIA OFFICE RE...|Common stock|     EUR|   2504379|2018-03-26|08:00|     12.25|   12.25|   12.25|   12.25|        1728|             3|\n",
      "|DE000A0LR936|     ST5|           STEICO SE|Common stock|     EUR|   2504382|2018-03-26|08:00|     22.35|   22.35|   22.35|   22.35|         334|             1|\n",
      "|DE000A0MZ4B0|     DLX|DELIGNIT AG      ...|Common stock|     EUR|   2504390|2018-03-26|08:00|      10.3|    10.3|    10.3|    10.3|         850|             1|\n",
      "|DE000A0Q8NC8|    ETLX|ETFS DAXGL.G.MIN....|         ETF|     EUR|   2504397|2018-03-26|08:00|    17.844|  17.844|  17.838|  17.838|        3085|             5|\n",
      "|DE000A0V9YU8|    4RT3|ETFS COM.SEC.DZ08...|         ETC|     EUR|   2504421|2018-03-26|08:00|    5.8895|  5.8895|  5.8895|  5.8895|           0|             1|\n",
      "|DE000A0WMPJ6|    AIXA|  AIXTRON SE NA O.N.|Common stock|     EUR|   2504428|2018-03-26|08:00|      16.8|    16.8|   16.75|  16.755|        3329|             8|\n",
      "|DE000A0Z2XN6|     RIB|RIB SOFTWARE SE  ...|Common stock|     EUR|   2504436|2018-03-26|08:00|     24.66|   24.66|   24.52|   24.52|       11741|            29|\n",
      "|DE000A0Z2ZZ5|    FNTN|  FREENET AG NA O.N.|Common stock|     EUR|   2504438|2018-03-26|08:00|     24.41|   24.42|   24.41|   24.42|         695|             6|\n",
      "|DE000A1A6V48|     KSC|      KPS AG NA O.N.|Common stock|     EUR|   2504441|2018-03-26|08:00|      9.15|    9.15|    9.15|    9.15|          73|             1|\n",
      "|DE000A1DAHH0|     BNR| BRENNTAG AG NA O.N.|Common stock|     EUR|   2504453|2018-03-26|08:00|     48.14|   48.14|   48.14|   48.14|         185|             2|\n",
      "|DE000A1EWWW0|     ADS|   ADIDAS AG NA O.N.|Common stock|     EUR|   2504471|2018-03-26|08:00|     196.3|  196.35|   196.3|  196.35|         591|            12|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path=os.path.join(os.getenv('V3IO_HOME_URL')+'/examples')\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(os.path.join(file_path)+'/stocks.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the spark DF into a table in Iguazio DB\n",
    "Specify the path to the NoSQL table that is associated with the DataFrame as a fully qualified path of the format v3io://container name/data path —\n",
    "where container name is the name of the table’s parent container, and data path is the path to the data within the specified container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the DB index key using the key option (note the key must be unique)\n",
    "df.write.format(\"io.iguaz.v3io.spark.sql.kv\").mode(\"append\").option(\"key\", \"ISIN\").option(\"allow-overwrite-schema\", \"true\").save(os.path.join(file_path)+'/stocks_tab/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a table via Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+--------+----------+----------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|Currency|SecurityID|      Date| Time|StartPrice|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "|DE0007461006|     TPE|   PVA TEPLA AG O.N.|Common stock|     EUR|   2505100|2018-03-26|08:30|      16.9|    16.9|    16.9|    16.9|         638|             2|\n",
      "|DE0008232125|     LHA|LUFTHANSA AG VNA ...|Common stock|     EUR|   2505130|2018-03-26|08:00|      25.6|   25.64|    25.6|   25.64|        3963|             7|\n",
      "|DE000A1681X5|     SNG|SINGULUS TECHNOL....|Common stock|     EUR|   2504610|2018-03-26|08:01|     12.78|   12.78|   12.76|   12.76|         467|             2|\n",
      "|FR0010527275|    LYM8|LYX.WORLD WATER U...|         ETF|     EUR|   2505248|2018-03-26|08:01|     34.06|   34.06|   34.06|   34.06|         100|             1|\n",
      "|IE00B910VR50|    ZPRE|SPDR MSCI EMU UCI...|         ETF|     EUR|   2505791|2018-03-26|08:31|     46.85|  46.855|   46.85|  46.855|        2677|             2|\n",
      "|DE0005313704|     AFX|CARL ZEISS MEDITE...|Common stock|     EUR|   2504922|2018-03-26|08:00|      50.2|    50.2|    50.2|    50.2|         164|             2|\n",
      "|DE000A0KEXC7|     V3S|VECTRON SYSTEMS  ...|Common stock|     EUR|   2504348|2018-03-26|08:14|     22.55|   22.55|   22.55|   22.55|          40|             1|\n",
      "|DE0005118806|     TGT|11 88 0 SOLUTIONS AG|Common stock|     EUR|   2504886|2018-03-26|08:12|     1.085|   1.085|   1.085|   1.085|        1200|             1|\n",
      "|LU0659580079|    XMK9|XTR.MSCI JAPAN 4CEOH|         ETF|     EUR|   2506044|2018-03-26|08:04|     20.08|   20.08|  20.079|  20.079|        3729|             2|\n",
      "|US0846707026|    BRYN|BERKSH. H.B NEW D...|Common stock|     EUR|   2506447|2018-03-26|08:00|    157.89|  157.99|  157.89|  157.89|         200|             3|\n",
      "|IE00BQN1K901|    CEMS|ISHSIV-E.MSCI EUR...|         ETF|     EUR|   2505478|2018-03-26|08:11|     5.624|   5.627|   5.624|   5.624|         105|             2|\n",
      "|DE000A2GS5D8|     DMP|DERMAPHARM HLDG I...|Common stock|     EUR|   2984169|2018-03-26|08:35|    25.465|  25.465|  25.465|  25.465|          26|             1|\n",
      "|DE0005140008|     DBK|DEUTSCHE BANK AG ...|Common stock|     EUR|   2504888|2018-03-26|08:00|     11.28|  11.284|  11.268|  11.284|       99093|            62|\n",
      "|DE0006289309|    EXX1|ISHS ESTXX BNKS.3...|         ETF|     EUR|   2505027|2018-03-26|08:03|    12.068|   12.07|  12.066|   12.07|       16274|             4|\n",
      "|US0727303028|    BAYA|      BAYER ADR /1/4|Common stock|     EUR|   2506445|2018-03-26|08:07|      22.6|    22.6|    22.6|    22.6|           2|             1|\n",
      "|US3696041033|     GEC|GENL EL. CO.     ...|Common stock|     EUR|   2506492|2018-03-26|08:21|     10.72|   10.72|   10.72|   10.72|         300|             1|\n",
      "|US0231351067|     AMZ|AMAZON.COM INC.  ...|Common stock|     EUR|   2506427|2018-03-26|08:00|      1230| 1231.88|    1230| 1231.88|           8|             2|\n",
      "|DE000A1H8MU2|     ADD|ADLER MODEMAERKTE...|Common stock|     EUR|   2504475|2018-03-26|08:29|       5.1|     5.1|     5.1|     5.1|         944|             1|\n",
      "|DE0005664809|     EVT|      EVOTEC AG O.N.|Common stock|     EUR|   2504963|2018-03-26|08:00|     15.93|   15.93|  15.895|  15.895|         484|             2|\n",
      "|DE0007274136|    STO3|STO SE+CO.KGAA VZ...|Common stock|     EUR|   2505092|2018-03-26|08:29|     114.6|   114.6|   114.6|   114.6|          62|             1|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+-----+----------+--------+--------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(os.path.join(file_path)+'/stocks_tab').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(os.path.join(file_path)+'/stocks_tab.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Schema-Overwrite Examples\n",
    "The following example creates a table named mytable with AttrA and AttrB attributes of type string and an AttrC attribute of type long, and then overwrites the table schema to change the type of AttrC to double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"a\", \"z\", 123),\n",
    "    (\"b\", \"y\", 456)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "df.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .save(os.path.join(file_path)+'/mytable/')\n",
    "    \n",
    "df = spark.createDataFrame([\n",
    "    (\"c\", \"x\", 32.12),\n",
    "    (\"d\", \"v\", 45.2)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "df.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .option(\"allow-overwrite-schema\", \"true\") \\\n",
    "    .save(os.path.join(file_path)+'/mytable/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a partition table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples creates a partitioned “weather” table  The option(\"partition\", \"year, month, day\") write option partitions the table by the year, month, and day item attributes. As demonstrated in the following image, if you browse the container in the dashboard after running the example, you’ll see that the weather directory has year=<value>/month=<value>/day=<value> partition directories that match the written items. If you select any of the nested day partition directories, you can see the written items and their attributes. For example, the first item (with attribute values 2016, 3, 25, 6, 16, 0.00, 55) is saved to a 20163256 file in a weather/year=2016/month=3/day=25 partition directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "table_path = os.path.join(os.getenv('V3IO_HOME_URL')+'/examples/weather/')\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (2016,  3, 25, 17, 18, 0.2, 62),\n",
    "    (2016,  7, 24,  7, 19, 0.0, 52),\n",
    "    (2016, 12, 24,  9, 10, 0.1, 47),\n",
    "    (2017,  5,  7, 14, 21, 0.0, 70),\n",
    "    (2017, 11,  1, 10, 15, 0.0, 34),\n",
    "    (2017, 12, 12, 16, 12, 0.0, 47),\n",
    "    (2017, 12, 24, 17, 11, 1.0, 50),\n",
    "    (2018,  1, 18, 17, 10, 2.0, 45),\n",
    "    (2018,  5, 20, 21, 20, 0.0, 59),\n",
    "    (2018, 11,  1, 11, 11, 0.1, 65)\n",
    "], [\"year\", \"month\", \"day\", \"hour\", \"degrees_cel\", \"rain_ml\", \"humidity_per\"])\n",
    "df_with_key = df.withColumn(\n",
    "    \"time\", concat(df[\"year\"], df[\"month\"], df[\"day\"], df[\"hour\"]))\n",
    "df_with_key.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"time\") \\\n",
    "    .option(\"partition\", \"year, month, day, hour\") \\\n",
    "    .save(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from partition table\n",
    "Following is the output of the example’s show commands for each read. The filtered results are gathered by scanning only the partition directories that match the filter criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full table read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2016|    3| 25|  17|         18|    0.2|          62| 201632517|\n",
      "|2016|    7| 24|   7|         19|    0.0|          52|  20167247|\n",
      "|2017|   11|  1|  10|         15|    0.0|          34| 201711110|\n",
      "|2017|   12| 12|  16|         12|    0.0|          47|2017121216|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "|2017|    5|  7|  14|         21|    0.0|          70|  20175714|\n",
      "|2018|    1| 18|  17|         10|    2.0|          45| 201811817|\n",
      "|2018|   11|  1|  11|         11|    0.1|          65| 201811111|\n",
      "|2018|    5| 20|  21|         20|    0.0|          59| 201852021|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path)\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### month < 7 filter — retrieve all data for the first six months of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2016|    7| 24|   7|         19|    0.0|          52|  20167247|\n",
      "|2017|   11|  1|  10|         15|    0.0|          34| 201711110|\n",
      "|2017|   12| 12|  16|         12|    0.0|          47|2017121216|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "|2018|   11|  1|  11|         11|    0.1|          65| 201811111|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month > 6\")\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### month == 12 AND day == 24 filter — retrieve all hours on Dec 24 each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month == 12 AND day == 24\")\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### month > 6 AND hour >= 8 AND hour <= 20 filter — retrieve 08:00–20:00 data for every day in the last six months of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|     time|\n",
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "|2016|    3| 25|  17|         18|    0.2|          62|201632517|\n",
      "|2017|    5|  7|  14|         21|    0.0|          70| 20175714|\n",
      "|2018|    1| 18|  17|         10|    2.0|          45|201811817|\n",
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month < 7 AND hour >= 8 AND hour <= 20\")\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional update\n",
    "This example demonstrates how to conditionally update NoSQL table items by using the condition write option. Each write call in the example is followed by matching read and show calls to read and display the value of the updated item in the target table after the write operation.\n",
    "\n",
    "The first write command writes an item (row) to a “cars” table . The item’s reg_license primary-key (identity-column) attribute is set to 7843321, the mode attribute is set to “Honda”, and the odometer attribute is set to 29321. The overwrite save mode is used to overwrite the table if it already exists and create it otherwise. Reading the item from the table produces this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "|reg_license|model|odometer|\n",
      "+-----------+-----+--------+\n",
      "|    7843321|Honda|   29321|\n",
      "+-----------+-----+--------+\n",
      "\n",
      "+-----------+-----+--------+\n",
      "|reg_license|model|odometer|\n",
      "+-----------+-----+--------+\n",
      "|    7843321|Honda|   31718|\n",
      "+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 29321)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .mode(\"overwrite\").save(\"v3io://users/iguazio/examples/cars/\")\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(os.path.join(file_path)+'/cars/')\n",
    "readDF.show()\n",
    "\n",
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 31718)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .option(\"condition\", \"${odometer} > odometer\") \\\n",
    "    .mode(\"append\").save(\"v3io://users/iguazio/examples/cars/\")\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(os.path.join(file_path)+'/cars/')\n",
    "readDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL queries (using Presto)\n",
    "## Reading the stock_tab table using SQL after being written by Spark DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * presto://iguazio:***@presto-api-presto.default-tenant.app.dev34.lab.iguazeng.com:443/v3io?protocol=https\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>securitydesc</th>\n",
       "        <th>securitytype</th>\n",
       "        <th>time</th>\n",
       "        <th>isin</th>\n",
       "        <th>minprice</th>\n",
       "        <th>date</th>\n",
       "        <th>endprice</th>\n",
       "        <th>numberoftrades</th>\n",
       "        <th>mnemonic</th>\n",
       "        <th>currency</th>\n",
       "        <th>securityid</th>\n",
       "        <th>maxprice</th>\n",
       "        <th>tradedvolume</th>\n",
       "        <th>startprice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>XTR.MSCI JAPAN 4CEOH</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:04</td>\n",
       "        <td>LU0659580079</td>\n",
       "        <td>20.079</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>20.079</td>\n",
       "        <td>2</td>\n",
       "        <td>XMK9</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506044</td>\n",
       "        <td>20.08</td>\n",
       "        <td>3729</td>\n",
       "        <td>20.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>COMSTAGE-DIVDAX U.ETF I</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:04</td>\n",
       "        <td>LU0603933895</td>\n",
       "        <td>29.24</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>29.24</td>\n",
       "        <td>1</td>\n",
       "        <td>C003</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2504275</td>\n",
       "        <td>29.24</td>\n",
       "        <td>35</td>\n",
       "        <td>29.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>UBS-ETF-MSCI EMU S.C.EOAD</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:33</td>\n",
       "        <td>LU0671493277</td>\n",
       "        <td>100.2</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>100.2</td>\n",
       "        <td>1</td>\n",
       "        <td>UEFD</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506045</td>\n",
       "        <td>100.2</td>\n",
       "        <td>180</td>\n",
       "        <td>100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>UBS-ETF-MSCI WLD.S.R.DLAD</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:55</td>\n",
       "        <td>LU0629459743</td>\n",
       "        <td>72.57</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>72.57</td>\n",
       "        <td>1</td>\n",
       "        <td>UIMM</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506031</td>\n",
       "        <td>72.57</td>\n",
       "        <td>1080</td>\n",
       "        <td>72.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>COMST.-F.A.Z.IDX U.ETF I</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:55</td>\n",
       "        <td>LU0650624025</td>\n",
       "        <td>27.495</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>27.495</td>\n",
       "        <td>1</td>\n",
       "        <td>C006</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506038</td>\n",
       "        <td>27.495</td>\n",
       "        <td>431</td>\n",
       "        <td>27.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>XTR.MSCI PAKIST.SWAP 1CDL</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:04</td>\n",
       "        <td>LU0659579147</td>\n",
       "        <td>1.5438</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>1.5438</td>\n",
       "        <td>1</td>\n",
       "        <td>XBAK</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506041</td>\n",
       "        <td>1.5438</td>\n",
       "        <td>32388</td>\n",
       "        <td>1.5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>UBS E.-MSCI EMU S.R.EOAD</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:56</td>\n",
       "        <td>LU0629460675</td>\n",
       "        <td>88.42</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>88.42</td>\n",
       "        <td>1</td>\n",
       "        <td>UIMR</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506033</td>\n",
       "        <td>88.42</td>\n",
       "        <td>114</td>\n",
       "        <td>88.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>COMS.-MSCI EM.M.T.U.ETF I</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:11</td>\n",
       "        <td>LU0635178014</td>\n",
       "        <td>42.233</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>42.233</td>\n",
       "        <td>1</td>\n",
       "        <td>E127</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2506035</td>\n",
       "        <td>42.233</td>\n",
       "        <td>46</td>\n",
       "        <td>42.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>COMST.-SH.DAX TR U.ETF I</td>\n",
       "        <td>ETF</td>\n",
       "        <td>08:03</td>\n",
       "        <td>LU0603940916</td>\n",
       "        <td>23.75</td>\n",
       "        <td>2018-03-26</td>\n",
       "        <td>23.75</td>\n",
       "        <td>1</td>\n",
       "        <td>C004</td>\n",
       "        <td>EUR</td>\n",
       "        <td>2504276</td>\n",
       "        <td>23.75</td>\n",
       "        <td>1200</td>\n",
       "        <td>23.75</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('XTR.MSCI JAPAN 4CEOH', 'ETF', '08:04', 'LU0659580079', '20.079', '2018-03-26', '20.079', '2', 'XMK9', 'EUR', '2506044', '20.08', '3729', '20.08'),\n",
       " ('COMSTAGE-DIVDAX U.ETF I', 'ETF', '08:04', 'LU0603933895', '29.24', '2018-03-26', '29.24', '1', 'C003', 'EUR', '2504275', '29.24', '35', '29.24'),\n",
       " ('UBS-ETF-MSCI EMU S.C.EOAD', 'ETF', '08:33', 'LU0671493277', '100.2', '2018-03-26', '100.2', '1', 'UEFD', 'EUR', '2506045', '100.2', '180', '100.2'),\n",
       " ('UBS-ETF-MSCI WLD.S.R.DLAD', 'ETF', '08:55', 'LU0629459743', '72.57', '2018-03-26', '72.57', '1', 'UIMM', 'EUR', '2506031', '72.57', '1080', '72.57'),\n",
       " ('COMST.-F.A.Z.IDX U.ETF I', 'ETF', '08:55', 'LU0650624025', '27.495', '2018-03-26', '27.495', '1', 'C006', 'EUR', '2506038', '27.495', '431', '27.495'),\n",
       " ('XTR.MSCI PAKIST.SWAP 1CDL', 'ETF', '08:04', 'LU0659579147', '1.5438', '2018-03-26', '1.5438', '1', 'XBAK', 'EUR', '2506041', '1.5438', '32388', '1.5438'),\n",
       " ('UBS E.-MSCI EMU S.R.EOAD', 'ETF', '08:56', 'LU0629460675', '88.42', '2018-03-26', '88.42', '1', 'UIMR', 'EUR', '2506033', '88.42', '114', '88.42'),\n",
       " ('COMS.-MSCI EM.M.T.U.ETF I', 'ETF', '08:11', 'LU0635178014', '42.233', '2018-03-26', '42.233', '1', 'E127', 'EUR', '2506035', '42.233', '46', '42.233'),\n",
       " ('COMST.-SH.DAX TR U.ETF I', 'ETF', '08:03', 'LU0603940916', '23.75', '2018-03-26', '23.75', '1', 'C004', 'EUR', '2504276', '23.75', '1200', '23.75')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_path = os.path.join('v3io.users.\"'+os.getenv('V3IO_USERNAME')+'/examples/stocks_tab\"')\n",
    "%sql select * from $table_path where securitytype ='ETF' and isin like 'LU06%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Data\n",
    "When you are done - cleaning the directory will be done by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unmark the comment\n",
    "# !rm -rf $HOME/examples/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to release compute and memory resources taken by spark we recommend running the following command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
