{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL and Analytics on Iguazio\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Set Up](#Set-Up)\n",
    "2. [Inititate a Spark Session](#Inititate-a-Spark-Session)\n",
    "3. [Load Data into Spark DataFrame](#Load-Data)\n",
    "  1. [Load Data from S3](#Load-Data-from-S3)\n",
    "  2. [Load Data from External Table](#Load-Data-from-External-Table)\n",
    "  3. [Load Data from UnStructure File](#Load-Data-from-UnStructure-File)\n",
    "  4. [Overwrite Table Schema](#Overwrite-Table-Schema)\n",
    "4. [Spark SQL](#Spark-SQL)\n",
    "  1. [Spark SQL on an Object](#Spark-SQL-on-an-Object)\n",
    "  2. [Spark SQL on a Table](#Spark-SQL-on-a-Table)\n",
    "  3. [Spark SQL on Iguazio](#Spark-SQL-on-Iguazio)\n",
    "  4. [Spark SQL Join](#Spark-SQL-Join)\n",
    "  5. [Spark SQL on a Parque File](#Spark-SQL-on-a-Parque-File)\n",
    "  6. [Spark SQL on a Partition Table](#Spark-SQL-on-a-Partition-Table)\n",
    "5. [Conditional Update on Iguazio](#Conditional-Update-on-Iguazio)\n",
    "6. [Clean Up](#Clean-Up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "**Spark SQL** is Apache Spark's module for working with structured data.  Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API.  DataFrames and SQL provide a common way to access a variety of data sources. <br>\n",
    "\n",
    "In this notebook, let's walk through how Spark SQL and DataFrames access objects, tables, unstructured data persisted on [**Iguazio Data Flatform**](https://www.iguazio.com/). <br>\n",
    "\n",
    "Iguazio drivers for Spark implement the data-source API and allow `predicate push down` (the queries pass to Iguazio datastore that only returns the relevant data), this allow accelerated and high-speed access from Spark to data stored in **Iguazio Data Flatform/Fabric**. <br>\n",
    "\n",
    "For more details read [Spark SQL and DataFrames documentation](https://spark.apache.org/docs/2.3.1/sql-programming-guide.html)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Before preparing the data, there are some initial steps required for setting up environmentn variables. <br>\n",
    "\n",
    "* Iguazio environmentn variables\n",
    "* AWS Credintial and Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: V3IO_HOME=/v3io/users/iguazio\n",
      "env: AWS_USER='<your_aws_user>'\n",
      "env: AWS_PASSWORD='<your_aws_password>'\n",
      "env: BUCKET='<your_s3_bucket_name_here>'\n",
      "env: ACCESSKEY='<your_accesskey.'\n",
      "env: SECRETKEY='<your_secretkey>'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Iguazio env\n",
    "V3IO_USER = os.getenv('V3IO_USERNAME')\n",
    "%env V3IO_HOME = /v3io/users/iguazio\n",
    "\n",
    "# AWS Credintial and Bucket\n",
    "%env AWS_USER = '<your_aws_user>'\n",
    "%env AWS_PASSWORD = '<your_aws_password>'\n",
    "%env BUCKET = '<your_s3_bucket_name_here>'\n",
    "%env ACCESSKEY = '<your_accesskey.'\n",
    "%env SECRETKEY = '<your_secretkey>'\n",
    "\n",
    "# printenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Inititate a Spark Session\n",
    "\n",
    "For getting started, you can start with Spark default configurations.\n",
    "\n",
    "Other than using the default Spark configurations, you may need modify some Spark properties to suite for your applications.  Based on the nature of datasets and data models, the way how to access the data, as well as the hardware resources, apply different configurations may lead to differnt performance.  You will go through some experiements in the late [sections](#Experiments-of-SQL-performance-on-a-Partition-KV-Store).  <br>\n",
    "\n",
    "* Some primary Spark properties are: <br>\n",
    "    `spark.driver.cores` <br>\n",
    "    `spark.driver.memory` <br>\n",
    "    `spark.executor.cores` <br>\n",
    "    `spark.executor.memory` <br>\n",
    "    `spark.cores.max` <br>\n",
    "\n",
    "    `spark.python.profile` <br>\n",
    "    `spark.pyspark.python` <br>\n",
    "\n",
    "* Set up the follow Spark properties for accessing AWS S3 bucket: <br>\n",
    "    `spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem` <br>\n",
    "    `spark.hadoop.fs.s3a.access.key=ACCESSKEY` <br>\n",
    "    `spark.hadoop.fs.s3a.secret.key=SECRETKEY` <br>\n",
    "    `spark.hadoop.fs.s3a.fast.upload=true` <br>\n",
    "\n",
    "For more details read [Spark Properties](https://spark.apache.org/docs/2.3.1/configuration.html#dynamically-loading-spark-properties) <br>\n",
    "\n",
    "For further performance services and support, contact our [Professional Services](https://www.iguazio.com/support/). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Spark Configuration is : \n",
      "\n",
      "spark.app.name =  Spark Session with Default Configurations\n",
      "spark.driver.cores =  None\n",
      "spark.driver.memory =  None\n",
      "spark.executor.cores =  2\n",
      "spark.executor.memory =  None\n",
      "spark.cores.max =  None\n",
      "spark.python.profile =  None\n",
      "spark.pyspark.python =  None\n",
      "spark.hadoop.fs.s3a.impl =  None\n",
      "\n",
      "\n",
      "Change Spark Configuration to : \n",
      "\n",
      "spark.app.name =  Spark SQL for Analytics\n",
      "spark.driver.cores =  None\n",
      "spark.driver.memory =  2g\n",
      "spark.executor.cores =  2\n",
      "spark.executor.memory =  4g\n",
      "spark.cores.max =  3\n",
      "spark.python.profile =  true\n",
      "spark.pyspark.python =  true\n",
      "spark.hadoop.fs.s3a.impl =  org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "\n",
      "\n",
      "CPU times: user 173 ms, sys: 34.3 ms, total: 207 ms\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "# Initiate a Spark Session\n",
    "spark = SparkSession.builder.appName(\"Spark Session with Default Configurations\").getOrCreate()\n",
    "\n",
    "# Get the default configurations\n",
    "spark.sparkContext._conf.getAll()\n",
    "\n",
    "conf = spark.sparkContext._conf\n",
    "\n",
    "print('\\n\\nCurrent Spark Configuration is : \\n')\n",
    "\n",
    "print('spark.app.name = ', conf.get(\"spark.app.name\"))\n",
    "print('spark.driver.cores = ', conf.get(\"spark.driver.cores\"))\n",
    "print('spark.driver.memory = ', conf.get(\"spark.driver.memory\"))\n",
    "print('spark.executor.cores = ', conf.get(\"spark.executor.cores\"))\n",
    "print('spark.executor.memory = ', conf.get(\"spark.executor.memory\"))\n",
    "print('spark.cores.max = ', conf.get(\"spark.cores.max\"))\n",
    "\n",
    "print('spark.python.profile = ', conf.get(\"spark.python.profile\"))\n",
    "print('spark.pyspark.python = ', conf.get(\"spark.pyspark.python\"))\n",
    "print('spark.hadoop.fs.s3a.impl = ', conf.get(\"spark.hadoop.fs.s3a.impl\"))\n",
    "\n",
    "# Update the default configurations\n",
    "# Here is an example of using one m5.2xlarge (8cpu, 32G) as application node.\n",
    "'''\n",
    "conf = spark.sparkContext._conf\\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "'''\n",
    "conf = spark.sparkContext._conf\\\n",
    "    .setAll([('spark.app.name', 'Spark SQL for Analytics'), \\\n",
    "             # ('spark.driver.cores', '2'), \\                # Only in cluster mode.\n",
    "             ('spark.driver.memory','2g'), \n",
    "             ('spark.executor.cores', '2'), \\\n",
    "             ('spark.executor.memory', '4g'), \\\n",
    "             ('spark.cores.max', '3'), \\\n",
    "             ('spark.python.profile', 'true'), \\\n",
    "             ('spark.pyspark.python', 'true'), \\\n",
    "             (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")])\n",
    "\n",
    "# Stop the current Spark Session\n",
    "spark.sparkContext.stop()\n",
    "\n",
    "# Create a Spark Session with new configurations\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Spark SQL for Analytics - ipynb\").getOrCreate()\n",
    "\n",
    "print('\\n\\nChange Spark Configuration to : \\n')\n",
    "\n",
    "print('spark.app.name = ', conf.get(\"spark.app.name\"))\n",
    "print('spark.driver.cores = ', conf.get(\"spark.driver.cores\"))\n",
    "print('spark.driver.memory = ', conf.get(\"spark.driver.memory\"))\n",
    "print('spark.executor.cores = ', conf.get(\"spark.executor.cores\"))\n",
    "print('spark.executor.memory = ', conf.get(\"spark.executor.memory\"))\n",
    "print('spark.cores.max = ', conf.get(\"spark.cores.max\"))\n",
    "\n",
    "print('spark.python.profile = ', conf.get(\"spark.python.profile\"))\n",
    "print('spark.pyspark.python = ', conf.get(\"spark.pyspark.python\"))\n",
    "print('spark.hadoop.fs.s3a.impl = ', conf.get(\"spark.hadoop.fs.s3a.impl\"))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Load Data\n",
    "\n",
    "The Spark Data Sources API supports a pluggable mechanism for integration with structured data-sources.  It is a unified API designed to support two major operations:\n",
    "\n",
    "1. Loading structured data from an external data source into Spark\n",
    "2. Storing structured data from Spark into an external data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from S3\n",
    "\n",
    "Load a file from S3 to Spark DataFrame <br>\n",
    "File URL of the form `s3a://bucket/path/to/file` <br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "df1 = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3a://iguazio-sample-data.s3.amazonaws.com/2018-03-26_BINS_XETR08.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Or Copy a file from AWS S3 to Iguazio\n",
    "Alternative, you can copy the data to Iguazio application node first. <br>\n",
    "Here, you copy a csv file from an AWS S3 to Iguazio. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  861k  100  861k    0     0  1116k      0 --:--:-- --:--:-- --:--:-- 1115k\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"iguazio-sample-data.s3.amazonaws.com/2018-03-26_BINS_XETR08.csv\" > ${V3IO_HOME}/stocks.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -alt /v3io/users/iguazio/stocks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move data into an Iguazio Data Container <br>\n",
    "Let's create a directory with name *stock*, then move data into Iguazio data container.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/02/28 08:08:57 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "mkdir: `v3io://users/iguazio/stock': File exists\n",
      "19/02/28 08:09:01 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "copyFromLocal: `v3io://users/iguazio/stock/stocks.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir v3io://users/iguazio/stock/\n",
    "!hadoop fs -copyFromLocal /v3io/users/iguazio/stocks.csv v3io://users/iguazio/stock/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in Iguazion Data Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/02/28 08:10:22 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "Found 1 items\n",
      "-rw-r--r--   1 50 nogroup     882055 2019-02-28 06:18 v3io://users/iguazio/stock/stocks.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls v3io://users/iguazio/stock/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark DataFrame, load a Iguazio file <br>\n",
    "\n",
    "Here, use Infer Schema to create a dataframe that infers the input schema automatically from data. <br>\n",
    "Also, you can specify a schema instead. <br>\n",
    "\n",
    "`schema = StructType(fields)` <br>\n",
    "`df = spark.read...option (\"Schema\", schema)....` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.35 ms, sys: 676 µs, total: 4.02 ms\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"v3io://users/iguazio/stock/stocks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISIN: string (nullable = true)\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- SecurityID: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- TradedVolume: integer (nullable = true)\n",
      " |-- NumberOfTrades: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ISIN',\n",
       " 'Mnemonic',\n",
       " 'SecurityDesc',\n",
       " 'SecurityType',\n",
       " 'Currency',\n",
       " 'SecurityID',\n",
       " 'Date',\n",
       " 'Time',\n",
       " 'StartPrice',\n",
       " 'MaxPrice',\n",
       " 'MinPrice',\n",
       " 'EndPrice',\n",
       " 'TradedVolume',\n",
       " 'NumberOfTrades']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Load Data from External Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MySQL as External Data Source\n",
    "Reading from MySQL as a bulk operation using pandas dataframe.\n",
    "\n",
    "**NOTE** If this notebook runs in AWS Cloud:\n",
    "AWS S3 provides **eventual consistency**.  Therefore, it takes time for users using the persisted data and softwar package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/39/15045ae46f2a123019aa968dfcba0396c161c20f855f11dea6796bcaae95/PyMySQL-0.9.3-py2.py3-none-any.whl (47kB)\n",
      "\u001b[K    100% |################################| 51kB 19.8MB/s \n",
      "\u001b[?25hInstalling collected packages: pymysql\n",
      "Successfully installed pymysql\n"
     ]
    }
   ],
   "source": [
    "# install MySQL lib\n",
    "!pip install pymysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfam_acc</th>\n",
       "      <th>rfam_id</th>\n",
       "      <th>auto_wiki</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>seed_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>RF03113</td>\n",
       "      <td>Poribacteria-1</td>\n",
       "      <td>2702</td>\n",
       "      <td>Poribacteria-1 RNA</td>\n",
       "      <td>Weinberg Z</td>\n",
       "      <td>Weinberg Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>RF03114</td>\n",
       "      <td>RT-1</td>\n",
       "      <td>2572</td>\n",
       "      <td>RT-1 RNA</td>\n",
       "      <td>Weinberg Z</td>\n",
       "      <td>Weinberg Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>RF03115</td>\n",
       "      <td>KDPG-aldolase</td>\n",
       "      <td>2703</td>\n",
       "      <td>KDPG-aldolase RNA</td>\n",
       "      <td>Weinberg Z</td>\n",
       "      <td>Weinberg Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rfam_acc         rfam_id  auto_wiki         description      author  \\\n",
       "3013  RF03113  Poribacteria-1       2702  Poribacteria-1 RNA  Weinberg Z   \n",
       "3014  RF03114            RT-1       2572            RT-1 RNA  Weinberg Z   \n",
       "3015  RF03115   KDPG-aldolase       2703   KDPG-aldolase RNA  Weinberg Z   \n",
       "\n",
       "     seed_source  \n",
       "3013  Weinberg Z  \n",
       "3014  Weinberg Z  \n",
       "3015  Weinberg Z  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pymysql\n",
    "import pandas as pd \n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=os.getenv('DB_HOST','mysql-rfam-public.ebi.ac.uk'),\n",
    "    port=int(4497),\n",
    "    user=os.getenv('DB_USER','rfamro'),\n",
    "    passwd=os.getenv('DB_PASSWORD',''),\n",
    "    db=os.getenv('DB_NAME','Rfam'),\n",
    "    charset='utf8mb4')\n",
    "\n",
    "dfMySQL = pd.read_sql_query(\"select rfam_acc,rfam_id,auto_wiki,description,author,seed_source FROM family\",\n",
    "    conn) \n",
    "\n",
    "dfMySQL.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Amazon Redshift as External Data Source\n",
    "\n",
    "The `spark-redshift` library is a data source API for [Amazon Redshift](https://aws.amazon.com/redshift/). <br>\n",
    "For this example let's create a S3 bucket `redshift-spark`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpS3Dir = \"s3n://redshift-spark/tmp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redshift Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshiftDBName = '<your_redshift_DB_name>'\n",
    "redshiftTableName = '<your_redshift_Table_name>'\n",
    "redshiftUserId = '<your_redshift_User_ID>'\n",
    "redshiftPassword = '<your_redshift_Password>'\n",
    "redshifturl = '<your_redshift_URL>'\n",
    "jdbcURL = s\"jdbc:redshift://$redshifturl/$redshiftDBName?user=$redshiftUserId&password=$redshiftPassword\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Redshift Table into Spark DF\n",
    "The `.format(\"com.databricks.spark.redshift\")` line tells the Data Sources API that we are using the `spark-redshift` package. <br>\n",
    "Enable `spark-redshift` to use the `tmpS3Dir` temporary location in S3 to store temporary files generated by `spark-redshift`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDSHFT = spark.read\n",
    "\t.format(\"com.databricks.spark.redshift\")\n",
    "\t.option(\"url\",jdbcURL )\n",
    "\t.option(\"tempdir\", tmpS3Dir)\n",
    "\t.option(\"dbtable\", redshiftTableName)\n",
    "\t.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Schema and Show a few Records\n",
    "`spark-redshift` automatically reads the schema from the Redshift table and maps its types back to Spark SQL's types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDSHFT.printSchema()\n",
    "dfRDSHFT.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist Redshift table data into Iguazio KV store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDSHFT = spark.write\\\n",
    "    .format(\"io.iguaz.v3io.spark.sql.kv\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"key\", key)\\\n",
    "    .option(\"sorting-key\", sorting-key)\\\n",
    "    .option(\"allow-overwrite-schema\", \"true\")\\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME'))+'/rdshfttable/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from UnStructure File"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Replace PATH_TO_A_JSON by the full URL of a JSON file, and remove the comment sign.\n",
    "# dfJSON = spark.read.json(\"PATH_TO_A_JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite Table Schema\n",
    "\n",
    "The following example creates a table named mytable with AttrA and AttrB attributes of type string and an AttrC attribute of type long, and then overwrites the table schema to change the type of AttrC to double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOWSchema = spark.createDataFrame([\n",
    "    (\"a\", \"z\", 123),\n",
    "    (\"b\", \"y\", 456)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "dfOWSchema.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME'))+'/mytable/')\n",
    "    \n",
    "dfOWSchema = spark.createDataFrame([\n",
    "    (\"c\", \"x\", 32.12),\n",
    "    (\"d\", \"v\", 45.2)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "dfOWSchema.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .option(\"allow-overwrite-schema\", \"true\") \\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME'))+'/mytable/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "Now, let's run some Spark SQL for analyze the stock dataset that was loaded to df DataFrame. <br>\n",
    "The fisrt a few SQLs list a few lines of selected columns in the dataset, as well as get some statistics of numerical columns. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Spark SQL on an Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|\n",
      "+------------+--------+--------------------+------------+\n",
      "|CH0038389992|    BBZA|BB BIOTECH NAM.  ...|Common stock|\n",
      "|CH0038863350|    NESR|NESTLE NAM.      ...|Common stock|\n",
      "|LU0378438732|    C001|COMSTAGE-DAX UCIT...|         ETF|\n",
      "+------------+--------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"ISIN\", \"Mnemonic\", \"SecurityDesc\", \"SecurityType\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive first a few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ISIN='CH0038389992', Mnemonic='BBZA', SecurityDesc='BB BIOTECH NAM.   SF 0,20'),\n",
       " Row(ISIN='CH0038863350', Mnemonic='NESR', SecurityDesc='NESTLE NAM.        SF-,10'),\n",
       " Row(ISIN='LU0378438732', Mnemonic='C001', SecurityDesc='COMSTAGE-DAX UCITS ETF I')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ISIN\", \"Mnemonic\", \"SecurityDesc\").head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Descriptive Statistics\n",
    "The function **describe** returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      TradedVolume|\n",
      "+-------+------------------+\n",
      "|  count|              7401|\n",
      "|   mean|3035.7574652074045|\n",
      "| stddev|18191.489026530675|\n",
      "|    min|                 0|\n",
      "|    max|            839200|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"TradedVolume\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Spark SQL on a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register as a Table for futher Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a few Columns and Only Print out a few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+\n",
      "|        ISIN|        SecurityDesc|SecurityID|\n",
      "+------------+--------------------+----------+\n",
      "|CH0038389992|BB BIOTECH NAM.  ...|   2504244|\n",
      "|CH0038863350|NESTLE NAM.      ...|   2504245|\n",
      "|LU0378438732|COMSTAGE-DAX UCIT...|   2504271|\n",
      "+------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = spark.sql(\"SELECT ISIN, SecurityDesc, SecurityID FROM stock limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Data to Find a Column or a few Columns for the Unique Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(ISIN)|\n",
      "+-----------+\n",
      "|       7401|\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|count(DISTINCT ISIN)|\n",
      "+--------------------+\n",
      "|                 737|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1 = spark.sql(\"SELECT COUNT(ISIN) FROM stock\").show()\n",
    "q2 = spark.sql(\"SELECT COUNT(DISTINCT(ISIN)) FROM stock\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(SecurityID)|\n",
      "+-----------------+\n",
      "|             7401|\n",
      "+-----------------+\n",
      "\n",
      "+--------------------------+\n",
      "|count(DISTINCT SecurityID)|\n",
      "+--------------------------+\n",
      "|                       737|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q4 = spark.sql(\"SELECT COUNT(SecurityID) FROM stock\").show()\n",
    "q5 = spark.sql(\"SELECT COUNT(DISTINCT(SecurityID)) FROM stock\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of ISIN, Date, Time can be the Unqiue Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|count(DISTINCT named_struct(ISIN, ISIN, Date, Date, Time, Time))|\n",
      "+----------------------------------------------------------------+\n",
      "|                                                            7401|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q6 = spark.sql(\"SELECT COUNT(DISTINCT(ISIN, Date, Time)) FROM stock\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Date and Time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(concat(Date, Time)='2018-03-26 00:00:0008:00'),\n",
       " Row(concat(Date, Time)='2018-03-26 00:00:0008:00')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(concat(col(\"Date\"), col(\"Time\"))).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2018, 3, 26, 0, 0), Time='08:00', datetime='2018-03-26 00:00:0008:00'),\n",
       " Row(Date=datetime.datetime(2018, 3, 26, 0, 0), Time='08:00', datetime='2018-03-26 00:00:0008:00'),\n",
       " Row(Date=datetime.datetime(2018, 3, 26, 0, 0), Time='08:00', datetime='2018-03-26 00:00:0008:00')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"datetime\", concat(col(\"Date\"), col(\"Time\")))\\\n",
    "    .select(\"Date\", \"Time\", \"datetime\").head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register another Table with a Unique Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"datetime\", concat(df[\"Date\"], df[\"Time\"])).createOrReplaceTempView(\"stock_UUID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Key is Unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|count(DISTINCT named_struct(ISIN, ISIN, datetime, datetime))|\n",
      "+------------------------------------------------------------+\n",
      "|                                                        7401|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q7 = spark.sql(\"SELECT COUNT(DISTINCT(ISIN, datetime)) FROM stock_UUID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Distinct values on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT datetime)|\n",
      "+------------------------+\n",
      "|                      60|\n",
      "+------------------------+\n",
      "\n",
      "CPU times: user 2.57 ms, sys: 0 ns, total: 2.57 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q8 = spark.sql(\"SELECT COUNT(DISTINCT(datetime)) FROM stock_UUID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show that **All data in this stock dataset is of the same date.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 7.63 µs\n",
      "+--------------------+\n",
      "|count(DISTINCT Time)|\n",
      "+--------------------+\n",
      "|                  60|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "q9 = spark.sql(\"SELECT COUNT(DISTINCT(Time)) FROM stock_UUID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Spark SQL on Iguazio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist Data from Spark DF to an Iguazio KV store\n",
    "Specify the format to `io.iguaz.v3io.spark.sql.kv` <br>\n",
    "Specify the path to the NoSQL table that is associated with the DataFrame as a fully qualified path of the format `v3io://Data_Container/Path` — where Data_Container is the name of the table’s data container, and Path is the path to the data within the specified container. <br>\n",
    "Specify the key and sorting-key (optional). <br>\n",
    "* KEY = key.sorting-key <br>\n",
    "\n",
    "**NOTE**: The KEY must be unique. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.69 ms, sys: 0 ns, total: 5.69 ms\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# UUID = key.sorting-key\n",
    "# key: ISIN\n",
    "# sorting-key : Date + Time\n",
    "df.withColumn(\"datetime\", concat(df[\"Date\"], df[\"Time\"]))\\\n",
    "    .write\\\n",
    "    .format(\"io.iguaz.v3io.spark.sql.kv\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"key\", \"ISIN\")\\\n",
    "    .option(\"sorting-key\", \"datetime\")\\\n",
    "    .option(\"allow-overwrite-schema\", \"true\")\\\n",
    "    .save('v3io://users/iguazio/stock/stocks_kv/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a KV store from Iguazion to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load('v3io://users/iguazio/stock/stocks_kv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.05 ms, sys: 3.07 ms, total: 8.12 ms\n",
      "Wall time: 222 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ISIN='DE0006969603', datetime='2018-03-26 00:00:0008:16')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df2.select(\"ISIN\", \"datetime\").head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"stock_kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|        ISIN|sum(TradedVolume)|\n",
      "+------------+-----------------+\n",
      "|LU0488317024|               35|\n",
      "|DE0005570808|             3350|\n",
      "|FI0009000681|            51473|\n",
      "|DE000A1K0375|             2000|\n",
      "|DE000A0H08M3|             5394|\n",
      "+------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 1.58 ms, sys: 1.23 ms, total: 2.82 ms\n",
      "Wall time: 2.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "q10 = spark.sql(\"SELECT ISIN, SUM(TradedVolume) FROM stock_kv GROUP BY ISIN\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist Data to another Iguazio KV store with Partition\n",
    "\n",
    "Partions are firstly by `Date`, and then by `Time`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.63 ms, sys: 0 ns, total: 3.63 ms\n",
      "Wall time: 8.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# UUID = key.sorting-key\n",
    "# key: ISIN\n",
    "# partition : Date, time\n",
    "df.write\\\n",
    "    .format(\"io.iguaz.v3io.spark.sql.kv\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"key\", \"ISIN\")\\\n",
    "    .option(\"partition\", \"Date, Time\")\\\n",
    "    .save('v3io://users/iguazio/stock/stocks_kv_partition/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load another KV store from Iguazio to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load('v3io://users/iguazio/stock/stocks_kv_partition/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+\n",
      "|        ISIN|               Date| Time|\n",
      "+------------+-------------------+-----+\n",
      "|DE000A1X3XX4|2018-03-26 00:00:00|08:00|\n",
      "|DE000TLX1005|2018-03-26 00:00:00|08:00|\n",
      "|DE0005232805|2018-03-26 00:00:00|08:00|\n",
      "+------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 1.92 ms, sys: 2.33 ms, total: 4.25 ms\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df3.select(\"ISIN\", \"Date\", \"Time\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments of SQL performance on a Partition KV Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"stock_kv_partintion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This partition data model does not well support for the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 4 executor, 2core and 1G perexecutor, never return results\n",
    "# 1*m5.2xlarge: Spark 2 executor, 2core and 4G per executor, never return results\n",
    "#%debug\n",
    "#q11 = spark.sql(\"SELECT ISIN, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY ISIN\").show(5)\n",
    "#q11 = spark.sql(\"SELECT ISIN, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY ISIN\").explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Experiments\n",
    "The following experiments are about running the same query with different [Spark configuration](#Spark-Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 19.8 ms, sys: 2 ms, total: 21.8 ms\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 4 executor, 2core and 1G per executor\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 18.1 ms, sys: 4.03 ms, total: 22.1 ms\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 2 executor, 2core and 4G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 16.8 ms, sys: 4.96 ms, total: 21.7 ms\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 4 executor, 1core and 4G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 17.9 ms, sys: 4.51 ms, total: 22.4 ms\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 3 executor, 1core and 8G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 21.5 ms, sys: 2.86 ms, total: 24.4 ms\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 6 executor, 1core and 4G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 15.4 ms, sys: 7.37 ms, total: 22.7 ms\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 3 executor, 1core and 8G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 13.2 ms, sys: 8.05 ms, total: 21.2 ms\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 1 executor, 1core and 20G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfL = spark.createDataFrame([(\"2504271\", \"LU0378438732\")], [\"SecurityID\", \"ISIN\"])\n",
    "dfR = spark.createDataFrame([(\"2504271\", \"JOIN in Spark SQL\")], [\"SecurityID\", \"SQL Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----------------+\n",
      "|SecurityID|        ISIN|SecurityID|        SQL Query|\n",
      "+----------+------------+----------+-----------------+\n",
      "|   2504271|LU0378438732|   2504271|JOIN in Spark SQL|\n",
      "+----------+------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfJoin = dfL.join(dfR, dfL.SecurityID == dfR.SecurityID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfL.createOrReplaceTempView(\"t1\")\n",
    "dfR.createOrReplaceTempView(\"t2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----------------+\n",
      "|SecurityID|        ISIN|SecurityID|        SQL Query|\n",
      "+----------+------------+----------+-----------------+\n",
      "|   2504271|LU0378438732|   2504271|JOIN in Spark SQL|\n",
      "+----------+------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qJoin = spark.sql(\"SELECT * FROM t1, t2 where t1.SecurityID=t2.SecurityID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL on a Parque File\n",
    "### Persist Data into Iguazio Data Container in Parque format\n",
    "Use the same stock dataset to store in Parque format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.98 ms, sys: 322 µs, total: 2.3 ms\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(\"v3io://users/iguazio/stock/stocks_parq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPARQ = spark.read.parquet(\"v3io://users/iguazio/stock/stocks_parq/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ISIN='CH0038389992', Date=datetime.datetime(2018, 3, 26, 0, 0))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPARQ.select(\"ISIN\", \"Date\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Spark SQL on a Partition Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a partition table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples creates a partitioned “weather” table  The option(\"partition\", \"year, month, day\") write option partitions the table by the year, month, and day item attributes. As demonstrated in the following image, if you browse the container in the dashboard after running the example, you’ll see that the weather directory has year=<value>/month=<value>/day=<value> partition directories that match the written items. If you select any of the nested day partition directories, you can see the written items and their attributes. For example, the first item (with attribute values 2016, 3, 25, 6, 16, 0.00, 55) is saved to a 20163256 file in a weather/year=2016/month=3/day=25 partition directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = os.path.join(os.getenv('V3IO_HOME')+'/examples/weather/')\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (2016,  3, 25, 17, 18, 0.2, 62),\n",
    "    (2016,  7, 24,  7, 19, 0.0, 52),\n",
    "    (2016, 12, 24,  9, 10, 0.1, 47),\n",
    "    (2017,  5,  7, 14, 21, 0.0, 70),\n",
    "    (2017, 11,  1, 10, 15, 0.0, 34),\n",
    "    (2017, 12, 12, 16, 12, 0.0, 47),\n",
    "    (2017, 12, 24, 17, 11, 1.0, 50),\n",
    "    (2018,  1, 18, 17, 10, 2.0, 45),\n",
    "    (2018,  5, 20, 21, 20, 0.0, 59),\n",
    "    (2018, 11,  1, 11, 11, 0.1, 65)\n",
    "], [\"year\", \"month\", \"day\", \"hour\", \"degrees_cel\", \"rain_ml\", \"humidity_per\"])\n",
    "\n",
    "df_with_key = df.withColumn(\n",
    "    \"time\", concat(df[\"year\"], df[\"month\"], df[\"day\"], df[\"hour\"]))\n",
    "\n",
    "df_with_key.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"time\") \\\n",
    "    .option(\"partition\", \"year, month, day, hour\") \\\n",
    "    .save(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from partition table\n",
    "Following is the output of the example’s show commands for each read.  The filtered results are gathered by scanning only the partition directories that match the filter criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Full Table Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2016|    3| 25|  17|         18|    0.2|          62| 201632517|\n",
      "|2016|    7| 24|   7|         19|    0.0|          52|  20167247|\n",
      "|2017|   11|  1|  10|         15|    0.0|          34| 201711110|\n",
      "|2017|   12| 12|  16|         12|    0.0|          47|2017121216|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "|2017|    5|  7|  14|         21|    0.0|          70|  20175714|\n",
      "|2018|    1| 18|  17|         10|    2.0|          45| 201811817|\n",
      "|2018|   11|  1|  11|         11|    0.1|          65| 201811111|\n",
      "|2018|    5| 20|  21|         20|    0.0|          59| 201852021|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path)\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all data in the last six months of each year:\n",
    "Filter: month > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2016|    7| 24|   7|         19|    0.0|          52|  20167247|\n",
      "|2017|   11|  1|  10|         15|    0.0|          34| 201711110|\n",
      "|2017|   12| 12|  16|         12|    0.0|          47|2017121216|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "|2018|   11|  1|  11|         11|    0.1|          65| 201811111|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month > 6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all hours in Dec 24 of each year:\n",
    "Filter: month == 12 AND day == 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month == 12 AND day == 24\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data in the last six months of each year between 08:00–20:00 in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|     time|\n",
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "|2016|    3| 25|  17|         18|    0.2|          62|201632517|\n",
      "|2017|    5|  7|  14|         21|    0.0|          70| 20175714|\n",
      "|2018|    1| 18|  17|         10|    2.0|          45|201811817|\n",
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month < 7 AND hour >= 8 AND hour <= 20\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Conditional Update on Iguazio\n",
    "\n",
    "This example demonstrates how to conditionally update NoSQL table items by using the condition write option.  Each write call in the example is followed by matching read and show calls to read and display the value of the updated item in the target table after the write operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "The first write command writes an item (row) to a “cars” table . The item’s reg_license primary-key (identity-column) attribute is set to 7843321, the mode attribute is set to “Honda”, and the odometer attribute is set to `29321`.  The overwrite save mode is used to overwrite the table if it already exists and create it otherwise.  Reading the item from the table produces this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "|reg_license|model|odometer|\n",
      "+-----------+-----+--------+\n",
      "|    7843321|Honda|   29321|\n",
      "+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 29321)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME'))+'/cars/')\n",
    "\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(os.path.join(os.getenv('V3IO_HOME'))+'/cars/') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update on Condition\n",
    "\n",
    "Update the odometer to `31718` on the condition of the new odometer value is greater than the old one's. This is to catch up the up-to-date value of odometer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "|reg_license|model|odometer|\n",
      "+-----------+-----+--------+\n",
      "|    7843321|Honda|   31718|\n",
      "+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 31718)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .option(\"condition\", \"${odometer} > odometer\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME'))+'/cars/')\n",
    "\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(os.path.join(os.getenv('V3IO_HOME'))+'/cars/') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations** - you complete Sparl SQL and Anytics in a notebook on Iguazio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Clean Up\n",
    "\n",
    "Prior to exiting, let's do housekeep to release disk space, computation and memory resources taken by this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Data\n",
    "When you are done - cleaning the directory will be done by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmark the comment\n",
    "# !rm -rf $HOME/PATH/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Stop Spark Session\n",
    "In order to release compute and memory resources taken by Spark we recommend running the following command to stop the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
