{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL and Analytics on Iguazio\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Set Up](#Set-Up)\n",
    "2. [Initiate a Spark Session](#Initiate-a-Spark-Session)\n",
    "3. [Load Data into Spark DataFrame](#Load-Data)\n",
    "  1. [Load Data from S3](#Load-Data-from-S3)\n",
    "  2. [Load Data from External Table](#Load-Data-from-External-Table)\n",
    "  3. [Load Data from SemiStructure File](#Load-Data-from-SemiStructure-File)\n",
    "  4. [Load Data from UnStructure File](#Load-Data-from-UnStructure-File)\n",
    "  5. [Overwrite Table Schema](#Overwrite-Table-Schema)\n",
    "4. [Spark SQL](#Spark-SQL)\n",
    "  1. [Spark SQL on an Object](#Spark-SQL-on-an-Object)\n",
    "  2. [Spark SQL on a Table](#Spark-SQL-on-a-Table)\n",
    "  3. [Spark SQL on Iguazio](#Spark-SQL-on-Iguazio)\n",
    "  4. [Spark SQL Join](#Spark-SQL-Join)\n",
    "  5. [Spark SQL on a Parquet File](#Spark-SQL-on-a-Parquet-File)\n",
    "  6. [Spark SQL on a Partition Table](#Spark-SQL-on-a-Partition-Table)\n",
    "5. [Conditional Update on Iguazio](#Conditional-Update-on-Iguazio)\n",
    "6. [Clean Up](#Clean-Up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "**Spark SQL** is Apache Spark's module for working with structured data.  Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API.  DataFrames and SQL provide a common way to access a variety of data sources. <br>\n",
    "\n",
    "In this notebook, let's walk through how Spark SQL and DataFrames access objects, tables, unstructured data persisted on [**Iguazio Data Science Platform**](https://www.iguazio.com/). <br>\n",
    "\n",
    "Iguazio drivers for Spark implement the data-source API and allow `predicate push down` (the queries pass to Iguazio datastore that only returns the relevant data), this allow accelerated and high-speed access from Spark to data stored in **Iguazio Data Science Platform/Fabric**. <br>\n",
    "\n",
    "For more details read [Spark SQL and DataFrames documentation](https://spark.apache.org/docs/2.3.1/sql-programming-guide.html)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Before preparing the data, there are some initial steps required for setting up environmentn variables. <br>\n",
    "\n",
    "* Iguazio environmentn variables\n",
    "* AWS Credintial and Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DIR1=examples\n",
      "env: AWS_USER='<your_aws_user>'\n",
      "env: AWS_PASSWORD='<your_aws_password>'\n",
      "env: BUCKET='<your_s3_bucket_name_here>'\n",
      "env: ACCESSKEY='<your_accesskey.'\n",
      "env: SECRETKEY='<your_secretkey>'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory for stocks\n",
    "%env DIR1 = examples\n",
    "\n",
    "# AWS Credintial and Bucket\n",
    "%env AWS_USER = '<your_aws_user>'\n",
    "%env AWS_PASSWORD = '<your_aws_password>'\n",
    "%env BUCKET = '<your_s3_bucket_name_here>'\n",
    "%env ACCESSKEY = '<your_accesskey.'\n",
    "%env SECRETKEY = '<your_secretkey>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Initiate a Spark Session\n",
    "\n",
    "For getting started, you can start with Spark default configurations.\n",
    "\n",
    "Other than using the default Spark configurations, you may need modify some Spark properties to suite for your applications.  Based on the nature of datasets and data models, the way how to access the data, as well as the hardware resources, apply different configurations may lead to differnt performance.  You will go through some experiements in the late [sections](#Experiments-of-SQL-performance-on-a-Partition-KV-Store).  <br>\n",
    "\n",
    "* Some primary Spark properties are: <br>\n",
    "    `spark.driver.cores` <br>\n",
    "    `spark.driver.memory` <br>\n",
    "    `spark.executor.cores` <br>\n",
    "    `spark.executor.memory` <br>\n",
    "    `spark.cores.max` <br>\n",
    "\n",
    "    `spark.python.profile` <br>\n",
    "    `spark.pyspark.python` <br>\n",
    "\n",
    "* Set up the follow Spark properties for accessing AWS S3 bucket: <br>\n",
    "    `spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem` <br>\n",
    "    `spark.hadoop.fs.s3a.access.key=ACCESSKEY` <br>\n",
    "    `spark.hadoop.fs.s3a.secret.key=SECRETKEY` <br>\n",
    "    `spark.hadoop.fs.s3a.fast.upload=true` <br>\n",
    "\n",
    "For more details read [Spark Properties](https://spark.apache.org/docs/2.3.1/configuration.html#dynamically-loading-spark-properties) <br>\n",
    "\n",
    "For further performance services and support, contact our [Professional Services](https://www.iguazio.com/support/). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Current Spark Configuration is : \n",
      "\n",
      "spark.app.name =  Spark Session with Default Configurations\n",
      "spark.driver.cores =  None\n",
      "spark.driver.memory =  2g\n",
      "spark.executor.cores =  2\n",
      "spark.executor.memory =  4g\n",
      "spark.cores.max =  3\n",
      "spark.python.profile =  true\n",
      "spark.pyspark.python =  true\n",
      "spark.hadoop.fs.s3a.impl =  org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "\n",
      "\n",
      "Change Spark Configuration to : \n",
      "\n",
      "spark.app.name =  Spark SQL for Analytics\n",
      "spark.driver.cores =  None\n",
      "spark.driver.memory =  2g\n",
      "spark.executor.cores =  2\n",
      "spark.executor.memory =  4g\n",
      "spark.cores.max =  3\n",
      "spark.python.profile =  true\n",
      "spark.pyspark.python =  true\n",
      "spark.hadoop.fs.s3a.impl =  org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "\n",
      "\n",
      "CPU times: user 54.2 ms, sys: 13.3 ms, total: 67.5 ms\n",
      "Wall time: 695 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "# Initiate a Spark Session\n",
    "spark = SparkSession.builder.appName(\"Spark Session with Default Configurations\").getOrCreate()\n",
    "\n",
    "# Get the default configurations\n",
    "# conf = spark.sparkContext._conf.getAll()\n",
    "# print(conf)\n",
    "conf = spark.sparkContext._conf\n",
    "\n",
    "print('\\n\\nCurrent Spark Configuration is : \\n')\n",
    "\n",
    "print('spark.app.name = ', conf.get(\"spark.app.name\"))\n",
    "print('spark.driver.cores = ', conf.get(\"spark.driver.cores\"))\n",
    "print('spark.driver.memory = ', conf.get(\"spark.driver.memory\"))\n",
    "print('spark.executor.cores = ', conf.get(\"spark.executor.cores\"))\n",
    "print('spark.executor.memory = ', conf.get(\"spark.executor.memory\"))\n",
    "print('spark.cores.max = ', conf.get(\"spark.cores.max\"))\n",
    "\n",
    "print('spark.python.profile = ', conf.get(\"spark.python.profile\"))\n",
    "print('spark.pyspark.python = ', conf.get(\"spark.pyspark.python\"))\n",
    "print('spark.hadoop.fs.s3a.impl = ', conf.get(\"spark.hadoop.fs.s3a.impl\"))\n",
    "\n",
    "# Update the default configurations\n",
    "# Here is an example of using one m5.2xlarge (8cpu, 32G) as application node.\n",
    "'''\n",
    "conf = spark.sparkContext._conf\\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "'''\n",
    "conf = spark.sparkContext._conf\\\n",
    "    .setAll([('spark.app.name', 'Spark SQL for Analytics'), \\\n",
    "             # ('spark.driver.cores', '2'), \\                # Only in cluster mode.\n",
    "             ('spark.driver.memory','2g'), \n",
    "             ('spark.executor.cores', '2'), \\\n",
    "             ('spark.executor.memory', '4g'), \\\n",
    "             ('spark.cores.max', '3'), \\\n",
    "             ('spark.python.profile', 'true'), \\\n",
    "             ('spark.pyspark.python', 'true'), \\\n",
    "             (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")])\n",
    "\n",
    "# Stop the current Spark Session\n",
    "spark.sparkContext.stop()\n",
    "\n",
    "# Create a Spark Session with new configurations\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Spark SQL for Analytics - ipynb\").getOrCreate()\n",
    "\n",
    "print('\\n\\nChange Spark Configuration to : \\n')\n",
    "\n",
    "print('spark.app.name = ', conf.get(\"spark.app.name\"))\n",
    "print('spark.driver.cores = ', conf.get(\"spark.driver.cores\"))\n",
    "print('spark.driver.memory = ', conf.get(\"spark.driver.memory\"))\n",
    "print('spark.executor.cores = ', conf.get(\"spark.executor.cores\"))\n",
    "print('spark.executor.memory = ', conf.get(\"spark.executor.memory\"))\n",
    "print('spark.cores.max = ', conf.get(\"spark.cores.max\"))\n",
    "\n",
    "print('spark.python.profile = ', conf.get(\"spark.python.profile\"))\n",
    "print('spark.pyspark.python = ', conf.get(\"spark.pyspark.python\"))\n",
    "print('spark.hadoop.fs.s3a.impl = ', conf.get(\"spark.hadoop.fs.s3a.impl\"))\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Load Data\n",
    "\n",
    "The Spark Data Sources API supports a pluggable mechanism for integration with structured data-sources.  It is a unified API designed to support two major operations:\n",
    "\n",
    "1. Loading structured data from an external data source into Spark\n",
    "2. Storing structured data from Spark into an external data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from S3\n",
    "\n",
    "Load a file from S3 to Spark DataFrame <br>\n",
    "File URL of the form `s3a://bucket/path/to/file` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install botocore"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import botocore.session\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession\n",
    "    .builder\n",
    "    .config(\n",
    "        'spark.driver.extraClassPath', \n",
    "        '/spark/3rd_party/aws-java-sdk-1.11.335.jar:'\n",
    "        '/spark/3rd_party/hadoop-aws-2.8.4.jar')\n",
    "    .config('fs.s3a.access.key', credentials.access_key)\n",
    "    .config('fs.s3a.secret.key', credentials.secret_key)\n",
    "    .appName(\"Load from S3\")\n",
    "    .getOrCreate()\n",
    "    \n",
    "# S3 object 's3a://bucket/path/to/file'\n",
    "s3Obj = \"s3a://iguazio-sample-data.s3.amazonaws.com/2018-03-26_BINS_XETR08.csv\"\n",
    "\n",
    "# Load to Spark DF\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(s3Obj)\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Or Copy a file from AWS S3 to Iguazio\n",
    "Alternative, you can copy the data to Iguazio Data Container first. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory `stock` in default Data Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /v3io/${V3IO_HOME}/${DIR1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy a csv file from an AWS S3 to Iguazio as `stocks.csv`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  861k  100  861k    0     0   975k      0 --:--:-- --:--:-- --:--:--  974k\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"iguazio-sample-data.s3.amazonaws.com/2018-03-26_BINS_XETR08.csv\" > /v3io/${V3IO_HOME}/${DIR1}/stocks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List files in Iguazion Data Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxrwxrwx 2 50 nogroup      0 Mar 24 22:05 family_tab\n",
      "drwxrwxrwx 2 50 nogroup      0 Mar 24 22:06 family\n",
      "drwxrwxrwx 2 50 nogroup      0 Mar 24 22:27 mytable\n",
      "drwxrwxrwx 2 50 nogroup      0 Mar 24 22:29 stocks_kv\n",
      "drwxr-xr-x 2 50 nogroup      0 Mar 24 22:30 stocks_parq\n",
      "drwxr-xr-x 2 50 nogroup      0 Mar 24 22:31 examples\n",
      "drwxrwxrwx 2 50 nogroup      0 Mar 24 22:31 weather\n",
      "-rw-r--r-- 1 50 nogroup 882055 Mar 24 22:43 stocks.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -altr /v3io/${V3IO_HOME}/${DIR1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up source file path with filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getenv('V3IO_HOME_URL')+'/examples')\n",
    "file = os.path.join(file_path+'/stocks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark DataFrame, load a Iguazio file <br>\n",
    "\n",
    "Here, use Infer Schema to create a dataframe that infers the input schema automatically from data. <br>\n",
    "Also, you can specify a schema instead. <br>\n",
    "\n",
    "`schema = StructType(fields)` <br>\n",
    "`df = spark.read...option (\"Schema\", schema)....` <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 123 µs, total: 2.69 ms\n",
      "Wall time: 556 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISIN: string (nullable = true)\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- SecurityID: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- StartPrice: double (nullable = true)\n",
      " |-- MaxPrice: double (nullable = true)\n",
      " |-- MinPrice: double (nullable = true)\n",
      " |-- EndPrice: double (nullable = true)\n",
      " |-- TradedVolume: integer (nullable = true)\n",
      " |-- NumberOfTrades: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ISIN',\n",
       " 'Mnemonic',\n",
       " 'SecurityDesc',\n",
       " 'SecurityType',\n",
       " 'Currency',\n",
       " 'SecurityID',\n",
       " 'Date',\n",
       " 'Time',\n",
       " 'StartPrice',\n",
       " 'MaxPrice',\n",
       " 'MinPrice',\n",
       " 'EndPrice',\n",
       " 'TradedVolume',\n",
       " 'NumberOfTrades']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Load Data from External Table\n",
    "\n",
    "In this section,  let's walk through two examples:\n",
    "\n",
    "1. Use pymysql, Python MySQL client library and Pandas DataFrame to load data from MySQL\n",
    "2. Use Spark JDBC to read table from AWS Redshift\n",
    "\n",
    "\n",
    "For more details read [Reading From Exteranl Databases](ReadingFromExternalDBs.ipynb) and [Spark JDBC to Databases](SparkJDBCtoDBs.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Use MySQL as External Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Database Connection to MySQL\n",
    "Reading from MySQL as a bulk operation using pandas dataframe.\n",
    "\n",
    "**NOTE** If this notebook runs in AWS Cloud:\n",
    "AWS S3 provides **eventual consistency**.  Therefore, it takes time for users using the persisted data and softwar package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfam_acc</th>\n",
       "      <th>rfam_id</th>\n",
       "      <th>auto_wiki</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>seed_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>RF03113</td>\n",
       "      <td>Poribacteria-1</td>\n",
       "      <td>2702</td>\n",
       "      <td>Poribacteria-1 RNA</td>\n",
       "      <td>Weinberg Z</td>\n",
       "      <td>Weinberg Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>RF03114</td>\n",
       "      <td>RT-1</td>\n",
       "      <td>2572</td>\n",
       "      <td>RT-1 RNA</td>\n",
       "      <td>Weinberg Z</td>\n",
       "      <td>Weinberg Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>RF03115</td>\n",
       "      <td>KDPG-aldolase</td>\n",
       "      <td>2703</td>\n",
       "      <td>KDPG-aldolase RNA</td>\n",
       "      <td>Weinberg Z</td>\n",
       "      <td>Weinberg Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rfam_acc         rfam_id  auto_wiki         description      author  \\\n",
       "3013  RF03113  Poribacteria-1       2702  Poribacteria-1 RNA  Weinberg Z   \n",
       "3014  RF03114            RT-1       2572            RT-1 RNA  Weinberg Z   \n",
       "3015  RF03115   KDPG-aldolase       2703   KDPG-aldolase RNA  Weinberg Z   \n",
       "\n",
       "     seed_source  \n",
       "3013  Weinberg Z  \n",
       "3014  Weinberg Z  \n",
       "3015  Weinberg Z  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pymysql\n",
    "import pandas as pd \n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=os.getenv('DB_HOST','mysql-rfam-public.ebi.ac.uk'),\n",
    "    port=int(4497),\n",
    "    user=os.getenv('DB_USER','rfamro'),\n",
    "    passwd=os.getenv('DB_PASSWORD',''),\n",
    "    db=os.getenv('DB_NAME','Rfam'),\n",
    "    charset='utf8mb4')\n",
    "\n",
    "pdfMySQL = pd.read_sql_query(\"select rfam_acc,rfam_id,auto_wiki,description,author,seed_source FROM family\",\n",
    "    conn) \n",
    "\n",
    "pdfMySQL.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Spark DataFrame from Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMySQL = spark.createDataFrame(pdfMySQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display a few Records of family Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+-------------------+--------------------+--------------------+\n",
      "|rfam_acc|  rfam_id|auto_wiki|        description|              author|         seed_source|\n",
      "+--------+---------+---------+-------------------+--------------------+--------------------+\n",
      "| RF00001|  5S_rRNA|     1302|   5S ribosomal RNA|Griffiths-Jones S...|Szymanski et al, ...|\n",
      "| RF00002|5_8S_rRNA|     1303| 5.8S ribosomal RNA|Griffiths-Jones S...|Wuyts et al, Euro...|\n",
      "| RF00003|       U1|     1304|U1 spliceosomal RNA|Griffiths-Jones S...|Zwieb C, The uRNA...|\n",
      "| RF00004|       U2|     1305|U2 spliceosomal RNA|Griffiths-Jones S...|The uRNA database...|\n",
      "| RF00005|     tRNA|     1306|               tRNA|Eddy SR, Griffith...|             Eddy SR|\n",
      "+--------+---------+---------+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMySQL.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print family Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rfam_acc: string (nullable = true)\n",
      " |-- rfam_id: string (nullable = true)\n",
      " |-- auto_wiki: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- seed_source: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMySQL.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register as a Table for Spark SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMySQL.createOrReplaceTempView(\"family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Number of Records of family Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3016|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM family\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify If auto_wiki could be a Unique Key of family Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT auto_wiki)|\n",
      "+-------------------------+\n",
      "|                     1345|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(distinct(auto_wiki)) FROM family\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Use Amazon Redshift as External Data Source\n",
    "\n",
    "The `spark-redshift` library is a data source API for [Amazon Redshift](https://aws.amazon.com/redshift/). <br>\n",
    "\n",
    "**Spark driver to Redshift**: \n",
    "The Spark driver connects to Redshift via JDBC using a username and password. Redshift does not support the use of IAM roles to authenticate this connection.  <br>\n",
    "\n",
    "**Spark to S3**:\n",
    "S3 acts as a middleman to store bulk data when reading from or writing to Redshift. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMOVE THIS WHEN PRODUCTION ENV UPDATED**  <br> \n",
    "**NOTE** <br> \n",
    "In SPARK_HOME/conf/spark-default.conf file, add path of Spark JDBC drivers to the following two Spark properties :\n",
    "\n",
    "    spark.driver.extraClassPath\n",
    "    spark.executor.extraClassPath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create a S3 bucket `redshift-spark`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpS3Dir = \"s3n://redshift-spark/tmp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redshift Environment Setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "redshiftDBName = '<your_redshift_DB_name>'\n",
    "redshiftTableName = '<your_redshift_Table_name>'\n",
    "redshiftUserId = '<your_redshift_User_ID>'\n",
    "redshiftPassword = '<your_redshift_Password>'\n",
    "redshifturl = '<your_redshift_URL>'\n",
    "jdbcURL = s\"jdbc:redshift://$redshifturl/$redshiftDBName?user=$redshiftUserId&password=$redshiftPassword\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshiftDBName = '<your_redshift_DB_name>'\n",
    "redshiftTableName = '<your_redshift_Table_name>'\n",
    "redshiftUserId = '<your_redshift_User_ID>'\n",
    "redshiftPassword = '<your_redshift_Password>'\n",
    "redshifturl = '<your_redshift_URL>'\n",
    "jdbcURL = s\"jdbc:redshift://$redshifturl/$redshiftDBName?user=$redshiftUserId&password=$redshiftPassword\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Redshift Table into Spark DF\n",
    "The `.format(\"com.databricks.spark.redshift\")` line tells the Data Sources API that we are using the `spark-redshift` package. <br>\n",
    "Enable `spark-redshift` to use the `tmpS3Dir` temporary location in S3 to store temporary files generated by `spark-redshift`. <br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfRDSHFT = spark.read\n",
    "\t.format(\"com.databricks.spark.redshift\")\n",
    "\t.option(\"url\",jdbcURL )\n",
    "\t.option(\"tempdir\", tmpS3Dir)\n",
    "\t.option(\"dbtable\", redshiftTableName)\n",
    "\t.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDSHFT = spark.read\n",
    "\t.format(\"com.databricks.spark.redshift\")\n",
    "\t.option(\"url\",jdbcURL )\n",
    "\t.option(\"tempdir\", tmpS3Dir)\n",
    "\t.option(\"dbtable\", redshiftTableName)\n",
    "\t.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Schema and Show a few Records\n",
    "`spark-redshift` automatically reads the schema from the Redshift table and maps its types back to Spark SQL's types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDSHFT.printSchema()\n",
    "dfRDSHFT.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist Redshift table data into Iguazio KV store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRDSHFT = spark.write\\\n",
    "    .format(\"io.iguaz.v3io.spark.sql.kv\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"key\", key)\\\n",
    "    .option(\"sorting-key\", sorting-key)\\\n",
    "    .option(\"allow-overwrite-schema\", \"true\")\\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME'))+'/rdshfttable/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from SemiStructure File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace PATH_TO_A_JSON by the full URL of a JSON file, and remove the comment sign.\n",
    "# dfJSON = spark.read.json(\"PATH_TO_A_JSON\")\n",
    "\n",
    "jsonFile = os.path.join(os.getenv('V3IO_HOME_URL')+'/GettingStarted/mLines.json')\n",
    "\n",
    "dfJSON = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(jsonFile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- array: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- dict: struct (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |-- int: long (nullable = true)\n",
      " |-- string: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfJSON.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from UnStructure File\n",
    "<font color=red>NOTE</font>  Spark version 2.4 or higher supports loading images."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Replace PATH_TO_AN_IMAGE by the full URL of a JSON file, and remove the comment sign.\n",
    "# dfImage = spark.read.format(\"image\").option(\"dropInvalid\", true).load(\"PATH_TO_AN_IMAGE\")\n",
    "\n",
    "imageFile = os.path.join(os.getenv('V3IO_HOME_URL')+'/GettingStarted/CoffeeTime.jpg')\n",
    "\n",
    "dfImage = spark.read.format(\"image\").option(\"dropInvalid\", \"true\").load(imageFile)\n",
    "\n",
    "dfImage.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite Table Schema\n",
    "\n",
    "The following example creates a table named mytable with AttrA and AttrB attributes of type string and an AttrC attribute of type long, and then overwrites the table schema to change the type of AttrC to double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOWSchema = spark.createDataFrame([\n",
    "    (\"a\", \"z\", 123),\n",
    "    (\"b\", \"y\", 456)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "dfOWSchema.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .save(os.path.join(file_path)+'/mytable/')\n",
    "    \n",
    "dfOWSchema = spark.createDataFrame([\n",
    "    (\"c\", \"x\", 32.12),\n",
    "    (\"d\", \"v\", 45.2)\n",
    "], [\"AttrA\", \"AttrB\", \"AttrC\"])\n",
    "dfOWSchema.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"key\", \"AttrA\") \\\n",
    "    .option(\"allow-overwrite-schema\", \"true\") \\\n",
    "    .save(os.path.join(file_path)+'/mytable/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Spark SQL\n",
    "\n",
    "Now, let's run some Spark SQL for analyze the stock dataset that was loaded to df DataFrame. <br>\n",
    "The fisrt a few SQLs list a few lines of selected columns in the dataset, as well as get some statistics of numerical columns. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Spark SQL on an Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|\n",
      "+------------+--------+--------------------+------------+\n",
      "|CH0038389992|    BBZA|BB BIOTECH NAM.  ...|Common stock|\n",
      "|CH0038863350|    NESR|NESTLE NAM.      ...|Common stock|\n",
      "|LU0378438732|    C001|COMSTAGE-DAX UCIT...|         ETF|\n",
      "+------------+--------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"ISIN\", \"Mnemonic\", \"SecurityDesc\", \"SecurityType\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive first a few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ISIN='CH0038389992', Mnemonic='BBZA', SecurityDesc='BB BIOTECH NAM.   SF 0,20'),\n",
       " Row(ISIN='CH0038863350', Mnemonic='NESR', SecurityDesc='NESTLE NAM.        SF-,10'),\n",
       " Row(ISIN='LU0378438732', Mnemonic='C001', SecurityDesc='COMSTAGE-DAX UCITS ETF I')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ISIN\", \"Mnemonic\", \"SecurityDesc\").head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Descriptive Statistics\n",
    "The function **describe** returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|      TradedVolume|\n",
      "+-------+------------------+\n",
      "|  count|              7401|\n",
      "|   mean|3035.7574652074045|\n",
      "| stddev|18191.489026530675|\n",
      "|    min|                 0|\n",
      "|    max|            839200|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"TradedVolume\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Spark SQL on a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register as a Table for futher Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"stock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a few Columns and Only Print out a few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+\n",
      "|        ISIN|        SecurityDesc|SecurityID|\n",
      "+------------+--------------------+----------+\n",
      "|CH0038389992|BB BIOTECH NAM.  ...|   2504244|\n",
      "|CH0038863350|NESTLE NAM.      ...|   2504245|\n",
      "|LU0378438732|COMSTAGE-DAX UCIT...|   2504271|\n",
      "+------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = spark.sql(\"SELECT ISIN, SecurityDesc, SecurityID FROM stock limit 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Data to Find a Column or a few Columns for the Unique Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(ISIN)|\n",
      "+-----------+\n",
      "|       7401|\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|count(DISTINCT ISIN)|\n",
      "+--------------------+\n",
      "|                 737|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1 = spark.sql(\"SELECT COUNT(ISIN) FROM stock\").show()\n",
    "q2 = spark.sql(\"SELECT COUNT(DISTINCT(ISIN)) FROM stock\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(SecurityID)|\n",
      "+-----------------+\n",
      "|             7401|\n",
      "+-----------------+\n",
      "\n",
      "+--------------------------+\n",
      "|count(DISTINCT SecurityID)|\n",
      "+--------------------------+\n",
      "|                       737|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q4 = spark.sql(\"SELECT COUNT(SecurityID) FROM stock\").show()\n",
    "q5 = spark.sql(\"SELECT COUNT(DISTINCT(SecurityID)) FROM stock\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of ISIN, Date, Time can be the Unqiue Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|count(DISTINCT named_struct(ISIN, ISIN, Date, Date, Time, Time))|\n",
      "+----------------------------------------------------------------+\n",
      "|                                                            7401|\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q6 = spark.sql(\"SELECT COUNT(DISTINCT(ISIN, Date, Time)) FROM stock\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Date and Time columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(concat(Date, Time)='2018-03-26 00:00:0008:00'),\n",
       " Row(concat(Date, Time)='2018-03-26 00:00:0008:00')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(concat(col(\"Date\"), col(\"Time\"))).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2018, 3, 26, 0, 0), Time='08:00', datetime='2018-03-26 00:00:0008:00'),\n",
       " Row(Date=datetime.datetime(2018, 3, 26, 0, 0), Time='08:00', datetime='2018-03-26 00:00:0008:00'),\n",
       " Row(Date=datetime.datetime(2018, 3, 26, 0, 0), Time='08:00', datetime='2018-03-26 00:00:0008:00')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"datetime\", concat(col(\"Date\"), col(\"Time\")))\\\n",
    "    .select(\"Date\", \"Time\", \"datetime\").head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register another Table with a Unique Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"datetime\", concat(df[\"Date\"], df[\"Time\"])).createOrReplaceTempView(\"stock_UUID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Key is Unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|count(DISTINCT named_struct(ISIN, ISIN, datetime, datetime))|\n",
      "+------------------------------------------------------------+\n",
      "|                                                        7401|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q7 = spark.sql(\"SELECT COUNT(DISTINCT(ISIN, datetime)) FROM stock_UUID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Distinct values on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT datetime)|\n",
      "+------------------------+\n",
      "|                      60|\n",
      "+------------------------+\n",
      "\n",
      "CPU times: user 0 ns, sys: 2.05 ms, total: 2.05 ms\n",
      "Wall time: 488 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q8 = spark.sql(\"SELECT COUNT(DISTINCT(datetime)) FROM stock_UUID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show that **All data in this stock dataset is of the same date.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.53 µs\n",
      "+--------------------+\n",
      "|count(DISTINCT Time)|\n",
      "+--------------------+\n",
      "|                  60|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "q9 = spark.sql(\"SELECT COUNT(DISTINCT(Time)) FROM stock_UUID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Spark SQL on Iguazio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist Data from Spark DF to an Iguazio KV store\n",
    "Specify the format to `io.iguaz.v3io.spark.sql.kv` <br>\n",
    "Specify the path to the NoSQL table that is associated with the DataFrame as a fully qualified path of the format `v3io://Data_Container/Path` — where Data_Container is the name of the table’s data container, and Path is the path to the data within the specified container. <br>\n",
    "Specify the key and sorting-key (optional). <br>\n",
    "* KEY = key.sorting-key <br>\n",
    "\n",
    "**NOTE**: The KEY must be unique. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 867 µs, sys: 2.45 ms, total: 3.32 ms\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set KV store name\n",
    "kvStore = os.path.join(file_path+'/examples/stocks_kv')\n",
    "\n",
    "# UUID = key.sorting-key\n",
    "# key: ISIN\n",
    "# sorting-key : Date + Time\n",
    "df.withColumn(\"datetime\", concat(df[\"Date\"], df[\"Time\"]))\\\n",
    "    .write\\\n",
    "    .format(\"io.iguaz.v3io.spark.sql.kv\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"key\", \"ISIN\")\\\n",
    "    .option(\"sorting-key\", \"datetime\")\\\n",
    "    .option(\"allow-overwrite-schema\", \"true\")\\\n",
    "    .save(kvStore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a KV store from Iguazion to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(kvStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.52 ms, sys: 71 µs, total: 4.59 ms\n",
      "Wall time: 165 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(ISIN='GB00B128C026', datetime='2018-03-26 00:00:0008:52')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df2.select(\"ISIN\", \"datetime\").head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"stock_kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|        ISIN|sum(TradedVolume)|\n",
      "+------------+-----------------+\n",
      "|DE000A0H08M3|             5394|\n",
      "|LU0488317024|               35|\n",
      "|DE000A1K0375|             2000|\n",
      "|DE0005570808|             3350|\n",
      "|FI0009000681|            51473|\n",
      "+------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 728 µs, sys: 1.33 ms, total: 2.06 ms\n",
      "Wall time: 827 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "q10 = spark.sql(\"SELECT ISIN, SUM(TradedVolume) FROM stock_kv GROUP BY ISIN\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist Data to another Iguazio KV store with Partition\n",
    "\n",
    "Partions are firstly by `Date`, and then by `Time`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 ms, sys: 68 µs, total: 2.3 ms\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Set Partitioned KV store name\n",
    "kvStorePartition = os.path.join(file_path+'/examples/stocks_kv_partition')\n",
    "\n",
    "# UUID = key.sorting-key\n",
    "# key: ISIN\n",
    "# partition : Date, time\n",
    "df.write\\\n",
    "    .format(\"io.iguaz.v3io.spark.sql.kv\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"key\", \"ISIN\")\\\n",
    "    .option(\"partition\", \"Date, Time\")\\\n",
    "    .save(kvStorePartition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load another KV store from Iguazio to Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(kvStorePartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+\n",
      "|        ISIN|               Date| Time|\n",
      "+------------+-------------------+-----+\n",
      "|DE0006202005|2018-03-26 00:00:00|08:00|\n",
      "|DE0005439004|2018-03-26 00:00:00|08:00|\n",
      "|DE0005419105|2018-03-26 00:00:00|08:00|\n",
      "+------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "CPU times: user 2.1 ms, sys: 1.06 ms, total: 3.16 ms\n",
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df3.select(\"ISIN\", \"Date\", \"Time\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Experiments of SQL performance on a Partition KV Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"stock_kv_partintion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This partition data model does not well support for the following query."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 4 executor, 2core and 1G perexecutor, never return results\n",
    "# 1*m5.2xlarge: Spark 2 executor, 2core and 4G per executor, never return results\n",
    "%debug\n",
    "q11 = spark.sql(\"SELECT ISIN, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY ISIN\").show(5)\n",
    "q11 = spark.sql(\"SELECT ISIN, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY ISIN\").explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Experiments\n",
    "The following experiments are about running the same query with different [Spark configuration](#Spark-Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 638 µs, sys: 1.3 ms, total: 1.93 ms\n",
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 4 executor, 2core and 1G per executor\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 1.87 ms, sys: 268 µs, total: 2.14 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 2 executor, 2core and 4G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 786 µs, sys: 1.35 ms, total: 2.13 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 4 executor, 1core and 4G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 2.04 ms, sys: 0 ns, total: 2.04 ms\n",
      "Wall time: 716 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 3 executor, 1core and 8G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 2.38 ms, sys: 0 ns, total: 2.38 ms\n",
      "Wall time: 797 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 6 executor, 1core and 4G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 950 µs, sys: 1.04 ms, total: 1.99 ms\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 3 executor, 1core and 8G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               Date|sum(TradedVolume)|\n",
      "+-------------------+-----------------+\n",
      "|2018-03-26 00:00:00|         22467641|\n",
      "+-------------------+-----------------+\n",
      "\n",
      "CPU times: user 1.55 ms, sys: 512 µs, total: 2.06 ms\n",
      "Wall time: 808 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1*m5.2xlarge: Spark 1 executor, 1core and 20G per executor,\n",
    "\n",
    "q12 = spark.sql(\"SELECT Date, SUM(TradedVolume) FROM stock_kv_partintion GROUP BY Date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfL = spark.createDataFrame([(\"2504271\", \"LU0378438732\")], [\"SecurityID\", \"ISIN\"])\n",
    "dfR = spark.createDataFrame([(\"2504271\", \"JOIN in Spark SQL\")], [\"SecurityID\", \"SQL Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----------------+\n",
      "|SecurityID|        ISIN|SecurityID|        SQL Query|\n",
      "+----------+------------+----------+-----------------+\n",
      "|   2504271|LU0378438732|   2504271|JOIN in Spark SQL|\n",
      "+----------+------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfJoin = dfL.join(dfR, dfL.SecurityID == dfR.SecurityID).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfL.createOrReplaceTempView(\"t1\")\n",
    "dfR.createOrReplaceTempView(\"t2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----------------+\n",
      "|SecurityID|        ISIN|SecurityID|        SQL Query|\n",
      "+----------+------------+----------+-----------------+\n",
      "|   2504271|LU0378438732|   2504271|JOIN in Spark SQL|\n",
      "+----------+------------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qJoin = spark.sql(\"SELECT * FROM t1, t2 where t1.SecurityID=t2.SecurityID\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL on a Parquet File\n",
    "### Persist Data into Iguazio Data Container in Parquet format\n",
    "Use the same stock dataset to store in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 ms, sys: 463 µs, total: 1.86 ms\n",
      "Wall time: 542 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parqFile = os.path.join(file_path+'/examples/stocks_parq')\n",
    "\n",
    "df.write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(parqFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPARQ = spark.read.parquet(parqFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ISIN='CH0038389992', Date=datetime.datetime(2018, 3, 26, 0, 0))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPARQ.select(\"ISIN\", \"Date\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Spark SQL on a Partition Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a partition table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples creates a partitioned “weather” table  The option(\"partition\", \"year, month, day\") write option partitions the table by the year, month, and day item attributes. As demonstrated in the following image, if you browse the container in the dashboard after running the example, you’ll see that the weather directory has year=<value>/month=<value>/day=<value> partition directories that match the written items. If you select any of the nested day partition directories, you can see the written items and their attributes. For example, the first item (with attribute values 2016, 3, 25, 6, 16, 0.00, 55) is saved to a 20163256 file in a weather/year=2016/month=3/day=25 partition directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = os.path.join(os.getenv('V3IO_HOME_URL')+'/examples/weather/')\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (2016,  3, 25, 17, 18, 0.2, 62),\n",
    "    (2016,  7, 24,  7, 19, 0.0, 52),\n",
    "    (2016, 12, 24,  9, 10, 0.1, 47),\n",
    "    (2017,  5,  7, 14, 21, 0.0, 70),\n",
    "    (2017, 11,  1, 10, 15, 0.0, 34),\n",
    "    (2017, 12, 12, 16, 12, 0.0, 47),\n",
    "    (2017, 12, 24, 17, 11, 1.0, 50),\n",
    "    (2018,  1, 18, 17, 10, 2.0, 45),\n",
    "    (2018,  5, 20, 21, 20, 0.0, 59),\n",
    "    (2018, 11,  1, 11, 11, 0.1, 65)\n",
    "], [\"year\", \"month\", \"day\", \"hour\", \"degrees_cel\", \"rain_ml\", \"humidity_per\"])\n",
    "\n",
    "df_with_key = df.withColumn(\n",
    "    \"time\", concat(df[\"year\"], df[\"month\"], df[\"day\"], df[\"hour\"]))\n",
    "\n",
    "df_with_key.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"key\", \"time\") \\\n",
    "    .option(\"partition\", \"year, month, day, hour\") \\\n",
    "    .save(table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from partition table\n",
    "Following is the output of the example’s show commands for each read.  The filtered results are gathered by scanning only the partition directories that match the filter criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Full Table Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2016|    3| 25|  17|         18|    0.2|          62| 201632517|\n",
      "|2016|    7| 24|   7|         19|    0.0|          52|  20167247|\n",
      "|2017|   11|  1|  10|         15|    0.0|          34| 201711110|\n",
      "|2017|   12| 12|  16|         12|    0.0|          47|2017121216|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "|2017|    5|  7|  14|         21|    0.0|          70|  20175714|\n",
      "|2018|    1| 18|  17|         10|    2.0|          45| 201811817|\n",
      "|2018|   11|  1|  11|         11|    0.1|          65| 201811111|\n",
      "|2018|    5| 20|  21|         20|    0.0|          59| 201852021|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path)\n",
    "readDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all data in the last six months of each year:\n",
    "Filter: month > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2016|    7| 24|   7|         19|    0.0|          52|  20167247|\n",
      "|2017|   11|  1|  10|         15|    0.0|          34| 201711110|\n",
      "|2017|   12| 12|  16|         12|    0.0|          47|2017121216|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "|2018|   11|  1|  11|         11|    0.1|          65| 201811111|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month > 6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all hours in Dec 24 of each year:\n",
    "Filter: month == 12 AND day == 24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|      time|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "|2016|   12| 24|   9|         10|    0.1|          47| 201612249|\n",
      "|2017|   12| 24|  17|         11|    1.0|          50|2017122417|\n",
      "+----+-----+---+----+-----------+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month == 12 AND day == 24\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data in the last six months of each year between 08:00–20:00 in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "|year|month|day|hour|degrees_cel|rain_ml|humidity_per|     time|\n",
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "|2016|    3| 25|  17|         18|    0.2|          62|201632517|\n",
      "|2017|    5|  7|  14|         21|    0.0|          70| 20175714|\n",
      "|2018|    1| 18|  17|         10|    2.0|          45|201811817|\n",
      "+----+-----+---+----+-----------+-------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(table_path) \\\n",
    "    .filter(\"month < 7 AND hour >= 8 AND hour <= 20\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Conditional Update on Iguazio\n",
    "\n",
    "This example demonstrates how to conditionally update NoSQL table items by using the condition write option.  Each write call in the example is followed by matching read and show calls to read and display the value of the updated item in the target table after the write operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data\n",
    "\n",
    "The first write command writes an item (row) to a “cars” table . The item’s reg_license primary-key (identity-column) attribute is set to 7843321, the mode attribute is set to “Honda”, and the odometer attribute is set to `29321`.  The overwrite save mode is used to overwrite the table if it already exists and create it otherwise.  Reading the item from the table produces this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "|reg_license|model|odometer|\n",
      "+-----------+-----+--------+\n",
      "|    7843321|Honda|   29321|\n",
      "+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 29321)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME_URL'))+'/cars/')\n",
    "\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(os.path.join(os.getenv('V3IO_HOME_URL'))+'/cars/') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update on Condition\n",
    "\n",
    "Update the odometer to `31718` on the condition of the new odometer value is greater than the old one's. This is to catch up the up-to-date value of odometer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "|reg_license|model|odometer|\n",
      "+-----------+-----+--------+\n",
      "|    7843321|Honda|   31718|\n",
      "+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writeDF = spark.createDataFrame([(\"7843321\", \"Honda\", 31718)],\n",
    "                                [\"reg_license\", \"model\", \"odometer\"])\n",
    "\n",
    "writeDF.write.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .option(\"key\", \"reg_license\") \\\n",
    "    .option(\"condition\", \"${odometer} > odometer\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(os.path.join(os.getenv('V3IO_HOME_URL'))+'/cars/')\n",
    "\n",
    "readDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\") \\\n",
    "    .load(os.path.join(os.getenv('V3IO_HOME_URL'))+'/cars/') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green> **Congratulations!**</font> - you've completed Spark SQL and Analytics in a notebook on Iguazio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Clean Up\n",
    "\n",
    "Prior to exiting, let's do housekeeping to release disk space, computation and memory resources taken by this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Data\n",
    "When you are done - cleaning the directory will be done by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmark the comment of the following\n",
    "# rm -rf /v3io/${V3IO_HOME}/examples/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Stop Spark Session\n",
    "In order to release compute and memory resources taken by Spark, we recommend running the following command to stop the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
