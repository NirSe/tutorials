{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product2Vec Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Configurations\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product2VecSkipGram:\n",
    "    def __init__(self, data, cv_data, batch_size, num_skips, skip_window, vocabulary_size, embedding_size=32,\n",
    "                 num_negative_sampled=64, len_ratio = 0.5):\n",
    "        self.data = data\n",
    "        self.cv_data = cv_data\n",
    "        self.data_index = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.num_skips = num_skips\n",
    "        self.skip_window = skip_window\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_negative_sampled = num_negative_sampled\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.len_ratio = len_ratio\n",
    "        self.models_dir = os.path.join(os.getcwd(), 'models')\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        self.build_graph()\n",
    "\n",
    "    def predict(self, products):\n",
    "        result = []\n",
    "        for i in range(0, len(products), self.batch_size):\n",
    "            batch = products[i:i+self.batch_size]\n",
    "            batch = self.sess.run(self.gathered, feed_dict={self.train_inputs: batch})\n",
    "            result.append(batch)\n",
    "        return np.concatenate(result, axis=0)\n",
    "\n",
    "    def train(self, num_steps, cv_every_n_steps, cv_steps, lrs):\n",
    "        with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "            average_loss = 0\n",
    "            learning_rate = 1.0\n",
    "            current = executor.submit(self.generate_batch)\n",
    "            for step in range(num_steps):\n",
    "                if step in lrs:\n",
    "                    learning_rate = lrs[step]\n",
    "                batch_inputs, batch_labels = current.result()\n",
    "                current = executor.submit(self.generate_batch)\n",
    "                feed_dict = {self.train_inputs: batch_inputs,\n",
    "                             self.train_labels: batch_labels,\n",
    "                             self.learning_rate: learning_rate}\n",
    "\n",
    "                _, loss_val = self.sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "                average_loss += loss_val\n",
    "\n",
    "                if step % 2000 == 0:\n",
    "                    if step > 0:\n",
    "                        average_loss /= 2000\n",
    "                    print('Average loss at step ', step, ': ', average_loss)\n",
    "                    average_loss = 0\n",
    "                if step % cv_every_n_steps == 0:\n",
    "                    self.data = shuffle(self.data, random_state=0)\n",
    "                    self.save_model(step)\n",
    "                    cv_loss = 0\n",
    "                    for batch_inputs, batch_labels in self.generate_test(cv_steps):\n",
    "                        feed_dict = {self.train_inputs: batch_inputs,\n",
    "                                     self.train_labels: batch_labels,\n",
    "                                     self.learning_rate: learning_rate}\n",
    "                        loss_val = self.sess.run(self.loss, feed_dict=feed_dict)\n",
    "                        cv_loss += loss_val\n",
    "                    print('CV',cv_loss / cv_steps)\n",
    "\n",
    "    def save_model(self, step):\n",
    "        self.saver.save(self.sess, os.path.join(self.models_dir, 'prod2vec_skip_gram'), global_step=step)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.saver = tf.train.import_meta_graph(path)\n",
    "        self.saver.restore(self.sess, tf.train.latest_checkpoint(path))\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        # variables\n",
    "        embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "\n",
    "        softmax_weights = tf.Variable(tf.truncated_normal([self.embedding_size, self.vocabulary_size],\n",
    "                                                          stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "        softmax_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "        self.gathered = tf.gather(embeddings, self.train_inputs)\n",
    "\n",
    "        prediction = tf.matmul(self.gathered, softmax_weights) + softmax_biases\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.train_labels, logits=prediction))\n",
    "\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def inc(self):\n",
    "        self.data_index = (self.data_index + 1) % len(self.data)\n",
    "\n",
    "    def inc_cv(self, data_index):\n",
    "        return (data_index + 1) % len(self.cv_data)\n",
    "\n",
    "    def generate_batch(self):\n",
    "        batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        counter = 0\n",
    "        while counter < self.batch_size:\n",
    "            current = self.data.iloc[self.data_index]\n",
    "            if len(current) == 1:\n",
    "                warnings.warn(\"lenght is one\", RuntimeWarning)\n",
    "                self.inc()\n",
    "                continue\n",
    "\n",
    "            span = min(2 * self.skip_window + 1, len(current))\n",
    "\n",
    "            x = target = np.random.randint(0, len(current))\n",
    "\n",
    "            targets_to_avoid = [x]\n",
    "\n",
    "            for j in range(self.num_skips):  # target varies!!! X constant!\n",
    "                while target in targets_to_avoid and len(targets_to_avoid) != span:\n",
    "                    target = np.random.randint(0, span)\n",
    "                if len(targets_to_avoid) == span or counter == self.batch_size:\n",
    "                    break\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[counter] = current[x]\n",
    "                labels[counter] = current[target]\n",
    "                counter += 1\n",
    "            self.inc()\n",
    "\n",
    "        return batch, labels\n",
    "\n",
    "    def generate_test(self, num_steps):\n",
    "        data_index = 0\n",
    "        for _ in range(num_steps):\n",
    "            batch = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "            labels = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "\n",
    "            counter = 0\n",
    "            while counter < self.batch_size:\n",
    "                current = self.cv_data.iloc[data_index]\n",
    "                if len(current) == 1:\n",
    "                    warnings.warn(\"lenght is one\", RuntimeWarning)\n",
    "                    data_index = self.inc_cv(data_index)\n",
    "                    continue\n",
    "\n",
    "                span = min(2 * self.skip_window + 1, len(current))\n",
    "\n",
    "                x = target = np.random.randint(0, len(current))\n",
    "\n",
    "                targets_to_avoid = [x]\n",
    "\n",
    "                for j in range(self.num_skips):  # target varies!!! X constant!\n",
    "                    while target in targets_to_avoid and len(targets_to_avoid) != span:\n",
    "                        target = np.random.randint(0, span)\n",
    "                    if len(targets_to_avoid) == span or counter == self.batch_size:\n",
    "                        break\n",
    "                    targets_to_avoid.append(target)\n",
    "                    batch[counter] = current[x]\n",
    "                    labels[counter] = current[target]\n",
    "                    counter += 1\n",
    "                data_index = self.inc_cv(data_index)\n",
    "\n",
    "            yield batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model (Create embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial size 3214874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:106: RuntimeWarning: lenght is one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 :  10.944147109985352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:139: RuntimeWarning: lenght is one\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 10.938432931138303\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Data loading\n",
    "np.random.seed(2017)\n",
    "products = pd.read_csv(DATA_DIR + 'products.csv')\n",
    "df = pd.read_pickle(DATA_DIR + 'prod2vec.pkl').products\n",
    "print('initial size', len(df))\n",
    "\n",
    "df_train, df_cv = train_test_split(df, test_size=0.1, random_state=2017)\n",
    "batch_size = 1024\n",
    "rates = {100000: 0.5,\n",
    "         200000: 0.25,\n",
    "         500000: 0.1}\n",
    "model = Product2VecSkipGram(df_train, df_cv, batch_size, 1, 1, np.max(products.product_id) + 1)\n",
    "model.train(120001, 20000, len(df_cv) // batch_size, rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
