{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuclio - Data preperation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio config\n",
    "\n",
    "# Trigger\n",
    "spec.triggers.retrain.kind = \"cron\"\n",
    "spec.triggers.retrain.attributes.interval = \"1h\"\n",
    "\n",
    "# Base image\n",
    "spec.build.baseImage = \"python:3.6-jessie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio cmd -c\n",
    "\n",
    "############\n",
    "# installs #\n",
    "############\n",
    "\n",
    "# Utils\n",
    "pip install pyyaml\n",
    "pip install pyarrow\n",
    "pip install pandas\n",
    "\n",
    "# Igz DB\n",
    "pip install v3io_frames --upgrade\n",
    "\n",
    "# Function\n",
    "pip install dask[\"complete\"]\n",
    "pip install dask-ml\n",
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%nuclio env\n",
    "\n",
    "# DB Config\n",
    "V3IO_FRAMESD=${V3IO_FRAMESD}\n",
    "V3IO_USERNAME=${V3IO_USERNAME}\n",
    "V3IO_ACCESS_KEY=${V3IO_ACCESS_KEY}\n",
    "\n",
    "# Metrics\n",
    "METRICS_TABLE=netops_metrics\n",
    "\n",
    "# Features\n",
    "FEATURES_TABLE=netops_features\n",
    "\n",
    "# Parallelizem\n",
    "NUMBER_OF_SHARDS=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "# DB Connection\n",
    "import v3io_frames as v3f\n",
    "\n",
    "# Parallelization\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Function\n",
    "import dask_ml.model_selection as dcv\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamp(df):\n",
    "#     df['timestamp'] = df['timestamp'].dt.strftime('%Y%m%d %H:%M')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_from_tsdb(df):\n",
    "    df.index.names = ['timestamp', 'company', 'data_center', 'device']\n",
    "    df = df.reset_index()\n",
    "    df = normalize_timestamp(df)\n",
    "    df = dd.from_pandas(df, npartitions=context.shards)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(context):\n",
    "    df = context.v3f.read(backend='tsdb', query=f'select cpu_utilization, latency, packet_loss, throughput, is_error from {context.metrics_table}',\n",
    "                          start=f'now-2h', end='now', steps='1m', multi_index=True)\n",
    "    df = format_df_from_tsdb(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_featuers(context, df, window_size: int):\n",
    "    features = df.copy()\n",
    "    features['key'] = features.apply(lambda row: f'{row[\"company\"]}_{row[\"data_center\"]}_{row[\"device\"]}', axis=1, meta=features.compute().dtypes)\n",
    "    features.set_index('key')\n",
    "    features[\"cpu_utilization\"] = features.cpu_utilization.rolling(window=window_size).mean()\n",
    "    features[\"latency\"] = features.latency.rolling(window=window_size).mean()\n",
    "    features[\"packet_loss\"] = features.packet_loss.rolling(window=window_size).mean()\n",
    "    features[\"throughput\"] = features.throughput.rolling(window=window_size).mean()\n",
    "    features[\"is_error\"] = features.is_error.rolling(window=window_size).max()\n",
    "                                     \n",
    "    features = features.dropna()\n",
    "    features = features.drop_duplicates()\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_tsdb(context, features: pd.DataFrame):   \n",
    "    context.v3f.write('tsdb', context.features_table, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    \n",
    "    # Create our DB client\n",
    "    v3io_client = v3f.Client(address='http://' + os.getenv('V3IO_FRAMESD', 'framesd:8081'), \n",
    "                        container='bigdata', \n",
    "                        password=os.environ['V3IO_ACCESS_KEY'], \n",
    "                        user=os.environ['V3IO_USERNAME'])\n",
    "    setattr(context, 'v3f', v3io_client)\n",
    "    \n",
    "    # Create Dask client\n",
    "    dask_client = Client()\n",
    "    setattr(context, 'dask', dask_client)\n",
    "    \n",
    "    # Set time to train on\n",
    "    train_on_last = os.getenv('TRAIN_ON_LAST', '7d')\n",
    "    setattr(context, 'train_on_last', train_on_last)\n",
    "    \n",
    "    # Set training set size\n",
    "    train_set_size = float(os.getenv('TRAIN_SIZE', 0.7))\n",
    "    setattr(context, 'train_size', train_set_size)\n",
    "    \n",
    "    # Netops metrics table\n",
    "    setattr(context, 'metrics_table', os.getenv('METRICS_TABLE', 'netops_metrics'))\n",
    "    \n",
    "    # Netops feautres table\n",
    "    setattr(context, 'features_table', os.getenv('FEATURES_TABLE', 'netops_features'))\n",
    "    context.v3f.create('tsdb', context.features_table, attrs={'rate': '1/s'}, if_exists=1)\n",
    "    \n",
    "    # Dask shards / CV\n",
    "    setattr(context, 'shards', int(os.getenv('NUMBER_OF_SHARDS', 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context, event):\n",
    "    \n",
    "    # Get data\n",
    "    raw = get_data(context) \n",
    "    print('got raw data')\n",
    "    # Get minute features\n",
    "    minute = create_rolling_featuers(context, raw, 3)\n",
    "    print('created minute data')\n",
    "    # Get hour features\n",
    "    hour = create_rolling_featuers(context, raw, 3*60)\n",
    "    column_names = {'cpu_utilization': 'cpu_utilization_hourly',\n",
    "                    'latency': 'latency_hourly',\n",
    "                    'packet_loss': 'packet_loss_hourly',\n",
    "                    'throughput': 'throughput_hourly'}\n",
    "    hour = hour.rename(columns=column_names)\n",
    "    print('created hour data')\n",
    "    # Create feature vector from data sources\n",
    "    features_rm = raw.merge(minute, on=['timestamp', 'company', 'data_center', 'device'], suffixes=('_raw', '_minute'))\n",
    "    features_rm.compute()\n",
    "    print('merged raw')\n",
    "    features = features_rm.merge(hour, on=['timestamp', 'company', 'data_center', 'device'], suffixes=('_raw', '_hourly'))\n",
    "    features = features.compute()\n",
    "    print('merged hour')\n",
    "    # Save feature vector to TSDB\n",
    "    \n",
    "    # Drop key columns\n",
    "    features = features.reset_index(drop=True)\n",
    "    feature_cols = [col for col in features.columns if 'key' in col]\n",
    "    features = features.drop(feature_cols, axis=1)\n",
    "    print('dropped columns')\n",
    "    \n",
    "    # Fix indexes before saving\n",
    "    features = features.set_index(['timestamp', 'company', 'data_center', 'device'])\n",
    "    print('set indexes')\n",
    "    \n",
    "    # Save to TSDB\n",
    "    save_to_tsdb(context, features)\n",
    "    print('saved to TSDB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "init_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "# init_context(context)\n",
    "event = nuclio.Event(body='')\n",
    "output = handler(context, event)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /v3io/bigdata/netops_metrics_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
